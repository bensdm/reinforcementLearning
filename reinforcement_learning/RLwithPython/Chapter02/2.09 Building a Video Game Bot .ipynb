{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Video Game Bot using OpenAI Universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let us learn how to build a video game bot which plays car racing game. Our objective is\n",
    "that car has to move forward without getting stuck by any obstacles and hitting other cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import necessary libraries,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/universe/universe/runtimes/__init__.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  spec = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import universe \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we simulate our car racing environment by make function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-18 09:17:44,391] Making new env: flashgames.NeonRace-v0\n",
      "/home/b/anaconda3/envs/rl/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('flashgames.NeonRace-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-18 09:17:46,264] Writing logs to file: /tmp/universe-11590.log\n",
      "[2019-03-18 09:17:46,486] Ports used: dict_keys([])\n",
      "[2019-03-18 09:17:46,487] [0] Creating container: image=quay.io/openai/universe.flashgames:0.20.28. Run the same thing by hand as: docker run -p 5900:5900 -p 15900:15900 --privileged --cap-add SYS_ADMIN --ipc host quay.io/openai/universe.flashgames:0.20.28\n",
      "[2019-03-18 09:17:48,995] Remote closed: address=localhost:15900\n",
      "[2019-03-18 09:17:48,996] At least one sockets was closed by the remote. Sleeping 1s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m Setting VNC and rewarder password: openai\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [Mon Mar 18 08:17:49 UTC 2019] Waiting for /tmp/.X11-unix/X0 to be created (try 1/10)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Xvnc TigerVNC 1.7.0 - built Sep  8 2016 10:39:22\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Copyright (C) 1999-2016 TigerVNC Team and many others (see README.txt)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] See http://www.tigervnc.org for information on TigerVNC.\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Underlying X server release 11400000, The X.Org Foundation\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension VNC-EXTENSION\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension Generic Event Extension\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension SHAPE\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension MIT-SHM\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension XInputExtension\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension XTEST\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension BIG-REQUESTS\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension SYNC\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension XKEYBOARD\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension XC-MISC\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension XINERAMA\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension XFIXES\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension RENDER\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension RANDR\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension COMPOSITE\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension DAMAGE\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension MIT-SCREEN-SAVER\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension DOUBLE-BUFFER\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension RECORD\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension DPMS\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension X-Resource\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension XVideo\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension XVideo-MotionCompensation\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Initializing built-in extension GLX\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Mon Mar 18 08:17:49 2019\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  vncext:      VNC extension running!\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  vncext:      Listening for VNC connections on all interface(s), port 5900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  vncext:      created VNC server for screen 0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] [dix] Could not init font path element /usr/share/fonts/X11/Type1/, removing from list!\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] [dix] Could not init font path element /usr/share/fonts/X11/75dpi/, removing from list!\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] [dix] Could not init font path element /usr/share/fonts/X11/100dpi/, removing from list!\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m WebSocket server settings:\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m   - Listen on :5898\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m   - Flash security policy server\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m   - No SSL/TLS support (no cert file)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m   - proxying from :5898 to localhost:5900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [Mon Mar 18 08:17:49 UTC 2019] [/usr/local/bin/sudoable-env-setup] Disabling outbound network traffic for none\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:17:49,248] Launching system_diagnostics_logger.py, recorder_logdir=/tmp/demo\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:17:49,250] Launching reward_recorder.py, recorder_logdir=/tmp/demo\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:17:49,252] Launching vnc_recorder.py, recorder_logdir=/tmp/demo\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:17:49,258] PID 56 launched with command ['sudo', '-H', '-u', 'nobody', 'DISPLAY=:0', 'DBUS_SESSION_BUS_ADDRESS=/dev/null', '/app/universe-envs/controlplane/bin/controlplane.py', '--rewarder-port=15901']\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:17:49,272] init detected end of child process 59 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [reward_recorder] [2019-03-18 08:17:49,644] Listening on 0.0.0.0:15898\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [vnc_recorder] [2019-03-18 08:17:49,677] Listening on 0.0.0.0:5899\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,836] [INFO:root] Starting play_controlplane.py with the following: command=['/app/universe-envs/controlplane/bin/controlplane.py', '--rewarder-port=15901'] args=Namespace(bot_demonstration=False, demonstration=False, env_id=None, idle_timeout=None, integrator_mode=False, no_env=False, no_rewarder=False, no_scorer=False, no_vexpect=False, remotes='vnc://127.0.0.1:5900', rewarder_fps=60, rewarder_port=15901, verbosity=0) env=environ({'USER': 'nobody', 'SUDO_UID': '0', 'MAIL': '/var/mail/nobody', 'USERNAME': 'nobody', 'SUDO_GID': '0', 'SHELL': '/usr/sbin/nologin', 'LOGNAME': 'nobody', 'SUDO_USER': 'root', 'HOSTNAME': '4d921e3ef6eb', 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin', 'SUDO_COMMAND': '/app/universe-envs/controlplane/bin/controlplane.py --rewarder-port=15901', 'DBUS_SESSION_BUS_ADDRESS': '/dev/null', 'TERM': 'xterm', 'HOME': '/nonexistent', 'DISPLAY': ':0'})\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,836] [INFO:root] [EnvStatus] Changing env_state: None (env_id=None) -> None (env_id=None) (episode_id: 0->0, fps=60)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,836] [INFO:universe.rewarder.remote] Starting Rewarder on port=15901\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,838] [INFO:universe.extra.universe.wrappers.logger] Running VNC environments with Logger set to print_frequency=5. To change this, pass \"print_frequency=k\" or \"print_frequency=None\" to \"env.configure\".\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,840] [INFO:universe.remotes.hardcoded_addresses] No rewarder addresses were provided, so this env cannot connect to the remote's rewarder channel, and cannot send control messages (e.g. reset)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,840] [INFO:universe.envs.vnc_env] Using the golang VNC implementation\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,841] [INFO:universe.envs.vnc_env] Using VNCSession arguments: {'start_timeout': 7, 'fine_quality_level': 50, 'compress_level': 9, 'encoding': 'zrle', 'subsample_level': 2}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,841] [INFO:universe.envs.vnc_env] Printed stats will ignore clock skew. (This usually makes sense only when the environment and agent are on the same machine.)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,844] [INFO:universe.envs.vnc_env] [0] Connecting to environment: vnc://127.0.0.1:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://None/viewer/?password=openai\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,844] [INFO:universe.extra.universe.envs.vnc_env] [0] Connecting to environment details: vnc_address=127.0.0.1:5900 vnc_password=openai rewarder_address=None rewarder_password=openai\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m 2019/03/18 08:17:49 I0318 08:17:49.844406 61 gymvnc.go:417] [0:127.0.0.1:5900] opening connection to VNC server\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,844] [INFO:root] [EnvStatus] Changing env_state: None (env_id=None) -> resetting (env_id=None) (episode_id: 0->1, fps=60)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,844] [INFO:root] [MainThread] Env state: env_id=None episode_id=1\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  Connections: accepted: 127.0.0.1::43280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  SConnection: Client needs protocol version 3.8\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:49,844] [INFO:root] [MainThread] Writing None to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  SConnection: Client requests security type VncAuth(2)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian bgr888\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m 2019/03/18 08:17:49 I0318 08:17:49.851935 61 gymvnc.go:550] [0:127.0.0.1:5900] connection established\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-18 09:17:49,999] Remote closed: address=localhost:5900\n",
      "[2019-03-18 09:17:50,000] Remote closed: address=localhost:15900\n",
      "[2019-03-18 09:17:50,002] At least one sockets was closed by the remote. Sleeping 1s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:17:50,016] init detected end of child process 16 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [Mon Mar 18 08:17:50 UTC 2019] [/usr/local/bin/sudoable-env-setup] Disabling outbound network traffic for none\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:50,073] [INFO:gym_flashgames.launcher] [MainThread] Launching new Chrome process (attempt 0/10)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:50,073] [INFO:root] Replacing selenium_wrapper_server since we currently do it at every episode boundary\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:50,166] [selenium_wrapper_server] Calling webdriver.Chrome()\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Mon Mar 18 08:17:51 2019\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  Connections: accepted: 172.17.0.1::58378\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  Connections: closed: 172.17.0.1::58378 (Clean disconnection)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-18 09:17:51,022] Using the golang VNC implementation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager: Framebuffer updates: 0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:   Total: 0 rects, 0 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-18 09:17:51,024] Using VNCSession arguments: {'start_timeout': 7, 'encoding': 'tight', 'fine_quality_level': 50, 'subsample_level': 2}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:          0 B (1:-nan ratio)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:51,020] [INFO:universe.rewarder.remote] Client connecting: peer=tcp4:127.0.0.1:57722 observer=True\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:51,021] [INFO:universe.rewarder.remote] WebSocket connection established\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 172.17.0.1 - openai [18/Mar/2019:08:17:51 +0000] \"GET / HTTP/1.1\" 101 164 \"-\" \"-\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:51,023] [INFO:universe.rewarder.remote] WebSocket connection closed: connection was closed uncleanly (peer dropped the TCP connection without previous WebSocket closing handshake)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:51,023] [INFO:universe.rewarder.remote] [Twisted] Non-active client disconnected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-18 09:17:51,058] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  Connections: accepted: 172.17.0.1::58392\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  SConnection: Client needs protocol version 3.8\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  SConnection: Client requests security type VncAuth(2)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian bgr888\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:51,107] [INFO:universe.rewarder.remote] Client connecting: peer=tcp4:127.0.0.1:57736 observer=False\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:51,107] [INFO:universe.rewarder.remote] WebSocket connection established\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:17:51 [info] 64#64: *5 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:17:51 [info] 64#64: *6 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:51,495] [selenium_wrapper_server] Call to webdriver.Chrome() completed: 1.33s\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:51,496] [INFO:gym_flashgames.launcher] [MainThread] Navigating browser to url=http://localhost\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:17:52 [info] 64#64: *7 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:52,773] [INFO:root] [EnvStatus] Changing env_state: resetting (env_id=None) -> running (env_id=None) (episode_id: 1->1, fps=60)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:52,774] [INFO:root] [MainThread] Writing None to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m Manhole[1552897072.7774]: Patched <built-in function fork> and <built-in function fork>.\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m Manhole[1552897072.7777]: Manhole UDS path: /tmp/manhole-61\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m Manhole[1552897072.7777]: Waiting for new connection (in pid:61) ...\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:54,861] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=2.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=509329.8 vnc_pixels_ps[total]=502472.3 reward_lag=None rewarder_message_lag=None fps=25.12\n"
     ]
    }
   ],
   "source": [
    "env.configure(remotes=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:57,777] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.frame\": {\"calls\": 300, \"std\": \"31.02us\", \"mean\": \"16.79ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"65.93us\", \"mean\": \"141.88us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"142.95us\", \"mean\": \"317.08us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"173.03us\", \"mean\": \"16.16ms\"}} counters={\"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.03986710963455155, \"std\": 0.3808831609316492}} gauges={} (export_time=139.24us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:57,777] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.00s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<760 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n"
     ]
    }
   ],
   "source": [
    "observation_n = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us create variables for moving the car,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:17:59,877] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=0.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=0.0 vnc_pixels_ps[total]=0.0 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:02,793] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"rewarder.frame\": {\"calls\": 301, \"std\": \"24.43us\", \"mean\": \"16.80ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"55.96us\", \"mean\": \"136.39us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"111.63us\", \"mean\": \"298.68us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"155.76us\", \"mean\": \"16.17ms\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0, \"std\": 0.0}} gauges={} (export_time=97.99us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:02,793] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<803 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:04,894] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=0.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=0.0 vnc_pixels_ps[total]=0.0 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:07,793] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.frame\": {\"calls\": 300, \"std\": \"24.93us\", \"mean\": \"16.79ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"48.04us\", \"mean\": \"146.83us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"128.00us\", \"mean\": \"310.98us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"153.33us\", \"mean\": \"16.16ms\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.0, \"std\": 0.0}} gauges={} (export_time=124.22us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:07,794] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.00s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<803 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:09,911] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=0.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=0.0 vnc_pixels_ps[total]=0.0 reward_lag=None rewarder_message_lag=None fps=60.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-18 09:18:11,118] [0:localhost:5900] ntpdate -q -p 8 localhost call timed out after 20.0s; killing the subprocess. This is ok, but you could have more accurate timings by enabling UDP port 123 traffic to your env. (Alternatively, you can try increasing the timeout by setting environment variable UNIVERSE_NTPDATE_TIMEOUT=10.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *10 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *11 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *12 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *13 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *14 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *15 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *16 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *17 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *18 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *19 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,143] [INFO:universe.rewarder.remote] CONNECTION STATUS: Marking connection as active: observer=False peer=tcp4:127.0.0.1:57736 total_conns=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-18 09:18:11,154] [0:localhost:5900] Sending reset for env_id=flashgames.NeonRace-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,155] [INFO:universe.rewarder.remote] Received reset message: {'headers': {'message_id': 10, 'episode_id': '0', 'sent_at': 1552897091.1552637}, 'body': {'seed': None, 'fps': 60, 'env_id': 'flashgames.NeonRace-v0'}, 'method': 'v0.env.reset'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,160] [INFO:root] [EnvStatus] Changing env_state: running (env_id=None) -> resetting (env_id=flashgames.NeonRace-v0) (episode_id: 1->2, fps=60)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,160] [ERROR:root] Closing server (via subprocess.close()) and all chromes (via pkill chromedriver || :; pkill chrome || :)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,160] [INFO:root] [Rewarder] Blocking until env finishes resetting\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:18:11,169] init detected end of child process 118 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,171] [INFO:root] [EnvController] RESET CAUSE: changing out environments due to v0.env.reset (with episode_id=0): flashgames.NeonRace-v0 -> flashgames.NeonRace-v0 (new episode_id=2 fps=60)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,172] [INFO:root] [EnvController] Env state: env_id=flashgames.NeonRace-v0 episode_id=2\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,173] [INFO:root] [EnvController] Writing flashgames.NeonRace-v0 to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:18:11,175] init detected end of child process 133 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:18:11,176] init detected end of child process 361 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:18:11,177] init detected end of child process 339 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *9 client closed connection while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:11 [info] 64#64: *8 client 127.0.0.1 closed keepalive connection\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [Mon Mar 18 08:18:11 UTC 2019] [/usr/local/bin/sudoable-env-setup] Allowing outbound network traffic to non-private IPs for git-lfs. (Going to fetch files via git lfs.)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:18:11,387] init detected end of child process 121 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:18:11,388] init detected end of child process 129 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:18:11,388] init detected end of child process 130 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:18:11,388] init detected end of child process 132 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [unpack-lfs] [2019-03-18 08:18:11,405] Unpacking files for flashgames.NeonRace-v0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [unpack-lfs] [2019-03-18 08:18:11,445] Merged 5 files from /tmp/flashgames.NeonRace-v0/public -> /app/universe-envs/flashgames/build/public/flashgames.NeonRace-v0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [unpack-lfs] [2019-03-18 08:18:11,446] Merged 7 files from /tmp/flashgames.NeonRace-v0/private -> /app/universe-envs/flashgames/build/private/flashgames.NeonRace-v0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [unpack-lfs] [2019-03-18 08:18:11,446] Completed unpack for flashgames.NeonRace-v0 in 0.041s\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [Mon Mar 18 08:18:11 UTC 2019] [/usr/local/bin/sudoable-env-setup] [debug] unpack-lfs completed with status code: 0. Created completion file: /usr/local/openai/git-lfs/flashgames.NeonRace-v0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [Mon Mar 18 08:18:11 UTC 2019] [/usr/local/bin/sudoable-env-setup] Disabling outbound network traffic for flashgames.NeonRace-v0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,696] [INFO:gym_flashgames.launcher] [EnvController] Launching new Chrome process (attempt 0/10)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,696] [INFO:root] Replacing selenium_wrapper_server since we currently do it at every episode boundary\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:11,792] [selenium_wrapper_server] Calling webdriver.Chrome()\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:13 [info] 64#64: *20 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:13 [info] 64#64: *21 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:18:13 [info] 64#64: *22 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,099] [selenium_wrapper_server] Call to webdriver.Chrome() completed: 1.31s\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,099] [INFO:gym_flashgames.launcher] [EnvController] Navigating browser to url=http://localhost/flashgames.NeonRace-v0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,137] [INFO:root] [EnvController] Running command: /app/universe-envs/controlplane/bin/play_vexpect -e flashgames.NeonRace-v0 -r vnc://127.0.0.1:5900 -d\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,760] [play_vexpect] No rewarder addresses were provided, so this env cannot connect to the remote's rewarder channel, and cannot send control messages (e.g. reset)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,760] [play_vexpect] Using the golang VNC implementation\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,761] [play_vexpect] Using VNCSession arguments: {'compress_level': 0, 'encoding': 'zrle', 'start_timeout': 7, 'fine_quality_level': 50, 'subsample_level': 2}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,761] [play_vexpect] Printed stats will ignore clock skew. (This usually makes sense only when the environment and agent are on the same machine.)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,763] [play_vexpect] [0] Connecting to environment: vnc://127.0.0.1:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://None/viewer/?password=openai\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,763] [play_vexpect] [0] Connecting to environment details: vnc_address=127.0.0.1:5900 vnc_password=openai rewarder_address=None rewarder_password=openai\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m 2019/03/18 08:18:13 I0318 08:18:13.763461 680 gymvnc.go:417] [0:127.0.0.1:5900] opening connection to VNC server\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Mon Mar 18 08:18:13 2019\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  Connections: accepted: 127.0.0.1::43472\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  SConnection: Client needs protocol version 3.8\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  SConnection: Client requests security type VncAuth(2)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian bgr888\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m 2019/03/18 08:18:13 I0318 08:18:13.769494 680 gymvnc.go:550] [0:127.0.0.1:5900] connection established\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:18:13,914] [play_vexpect] Waiting for any of [initialize0] to activate\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m 2019/03/18 08:18:14 I0318 08:18:14.87829 61 gymvnc.go:374] [0:127.0.0.1:5900] update queue max of 60 reached; pausing further updates\n"
     ]
    }
   ],
   "source": [
    "# Move left\n",
    "left = [('KeyEvent', 'ArrowUp', True), ('KeyEvent', 'ArrowLeft', True),\n",
    "         ('KeyEvent', 'ArrowRight', False)]\n",
    "\n",
    "# Move right\n",
    "right = [('KeyEvent', 'ArrowUp', True), ('KeyEvent', 'ArrowLeft', False),\n",
    "         ('KeyEvent', 'ArrowRight', True)]\n",
    "\n",
    "# Move forward\n",
    "\n",
    "forward = [('KeyEvent', 'ArrowUp', True), ('KeyEvent', 'ArrowRight', False),\n",
    "            ('KeyEvent', 'ArrowLeft', False), ('KeyEvent', 'n', True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Followed by, we will initialize some other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use turn variable for deciding whether to turn or not\n",
    "turn = 0\n",
    "\n",
    "# We store all the rewards in rewards list\n",
    "rewards = []\n",
    "\n",
    "# we will use buffer as some kind of threshold\n",
    "buffer_size = 100\n",
    "\n",
    "# We set our initial action has forward i.e our car moves just forward without making any turns\n",
    "action = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let us begin our game agent to play in an infinite loop which continuously performs an action based on interaction with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-18 09:21:59,784] WARNING: returning more than 60 aggregated rewards: {'0': '76 (episode_id=2)'}. Either your agent is not keeping up with the framerate, or you should have called \".reset()\" to clear pending rewards and reset the environments to a known state.\n",
      "[2019-03-18 09:21:59,788] Throttle fell behind by 221.06s; lost 13263.82 frames\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "/tmp/pip-install-wa743vh1/go-vncdriver/.build/src/github.com/openai/go-vncdriver/main.go:387: go_vncdriver was installed without OpenGL support. See https://github.com/openai/go-vncdriver for details on how debug.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f4f11dce2175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/universe/universe/wrappers/render.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Could log, but no need.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/universe/universe/envs/vnc_env.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m'human'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvnc_session\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvnc_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: /tmp/pip-install-wa743vh1/go-vncdriver/.build/src/github.com/openai/go-vncdriver/main.go:387: go_vncdriver was installed without OpenGL support. See https://github.com/openai/go-vncdriver for details on how debug."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:01,544] [INFO:universe.rewarder.remote] [Rewarder] Over past 4.67s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:01,894] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=14.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=43821.4 vnc_pixels_ps[total]=68623.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:01,904] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 73, \"std\": \"20.97us\", \"mean\": \"94.07us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 13, \"std\": \"1.13ms\", \"mean\": \"8.66ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"21.09us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 73, \"std\": \"19.34us\", \"mean\": \"56.51us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"1.80ms\", \"mean\": \"15.70ms\"}, \"reward.parsing.gameover\": {\"calls\": 73, \"std\": \"67.30us\", \"mean\": \"173.16us\"}, \"reward.parsing.score\": {\"calls\": 73, \"std\": \"3.37ms\", \"mean\": \"1.87ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 29, \"std\": \"22.30us\", \"mean\": \"94.29us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"1.89ms\", \"mean\": \"810.26us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"43.14us\", \"mean\": \"122.45us\"}} counters={\"agent_conn.reward\": {\"calls\": 11, \"mean\": 0.9090909090909091, \"std\": 0.30151134457776363}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.24916943521594684, \"std\": 0.4483759299174594}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 44, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 60, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 73, \"mean\": 41.780821917808225, \"std\": 2.1873790464494864, \"value\": 51.0}} (export_time=111.10us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:02,571] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 30 reward messages to agent: reward=48.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 8273, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2076 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:03,603] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 28 reward messages to agent: reward=127.0 reward_min=3.0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 11630, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:04,672] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 32 reward messages to agent: reward=231.0 reward_min=6.0 reward_max=9.0 done=False info={'rewarder.vnc.updates.bytes': 8273, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:05,702] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=314.0 reward_min=9.0 reward_max=11.0 done=False info={'rewarder.vnc.updates.bytes': 12783, 'rewarder.vnc.updates.pixels': 4774, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:06,704] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 28 reward messages to agent: reward=380.0 reward_min=12.0 reward_max=38.0 done=False info={'rewarder.vnc.updates.bytes': 11630, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:06,911] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=32.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=291699.2 vnc_pixels_ps[total]=177754.1 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:06,912] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 162, \"std\": \"15.36us\", \"mean\": \"79.60us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 150, \"std\": \"1.07ms\", \"mean\": \"9.49ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"14.83us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 162, \"std\": \"9.78us\", \"mean\": \"41.84us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"5.15ms\", \"mean\": \"11.19ms\"}, \"reward.parsing.gameover\": {\"calls\": 162, \"std\": \"36.98us\", \"mean\": \"178.86us\"}, \"reward.parsing.score\": {\"calls\": 162, \"std\": \"2.73ms\", \"mean\": \"9.11ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 140, \"std\": \"11.50us\", \"mean\": \"75.23us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"5.09ms\", \"mean\": \"5.25ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"23.45us\", \"mean\": \"90.17us\"}} counters={\"agent_conn.reward\": {\"calls\": 146, \"mean\": 8.047945205479452, \"std\": 4.664365561738122}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5382059800664456, \"std\": 0.499368371809949}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 22, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 12, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 157, \"mean\": 466.8407643312101, \"std\": 348.7552868738895, \"value\": 1212.0}} (export_time=260.59us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:07,737] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 32 reward messages to agent: reward=452.0 reward_min=0 reward_max=15.0 done=False info={'rewarder.vnc.updates.bytes': 11630, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2080 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:08,771] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=502.0 reward_min=15.0 reward_max=17.0 done=False info={'rewarder.vnc.updates.bytes': 11630, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:09,777] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 31 reward messages to agent: reward=534.0 reward_min=17.0 reward_max=18.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:10,804] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 30 reward messages to agent: reward=464.0 reward_min=14.0 reward_max=18.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:11,837] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=514.0 reward_min=15.0 reward_max=17.0 done=False info={'rewarder.vnc.updates.bytes': 11589, 'rewarder.vnc.updates.pixels': 3850, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:11,927] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=32.1 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=350110.2 vnc_pixels_ps[total]=185359.5 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:11,929] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 162, \"std\": \"20.85us\", \"mean\": \"85.65us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 150, \"std\": \"1.16ms\", \"mean\": \"9.08ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"12.96us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 162, \"std\": \"6.95us\", \"mean\": \"39.70us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"4.95ms\", \"mean\": \"11.40ms\"}, \"reward.parsing.gameover\": {\"calls\": 162, \"std\": \"34.59us\", \"mean\": \"181.32us\"}, \"reward.parsing.score\": {\"calls\": 162, \"std\": \"2.66ms\", \"mean\": \"8.74ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 151, \"std\": \"11.44us\", \"mean\": \"74.28us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"4.90ms\", \"mean\": \"5.07ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.69us\", \"mean\": \"97.47us\"}} counters={\"agent_conn.reward\": {\"calls\": 151, \"mean\": 16.0, \"std\": 1.6931233465600395}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5382059800664448, \"std\": 0.499368371809949}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 11, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 12, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 162, \"mean\": 2416.5987654320975, \"std\": 708.9911777905295, \"value\": 3642.0}} (export_time=220.06us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:14,670] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.83s, sent 5 reward messages to agent: reward=65.0 reward_min=1.0 reward_max=17.0 done=False info={'rewarder.vnc.updates.bytes': 10603, 'rewarder.vnc.updates.pixels': 4054, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2069 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:15,701] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=47.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 8101, 'rewarder.vnc.updates.pixels': 3214, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:16,704] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=98.0 reward_min=2.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 7733, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:16,936] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 155, \"std\": \"26.06us\", \"mean\": \"96.06us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 71, \"std\": \"1.01ms\", \"mean\": \"9.23ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"17.93us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 155, \"std\": \"22.22us\", \"mean\": \"56.14us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"4.10ms\", \"mean\": \"13.74ms\"}, \"reward.parsing.gameover\": {\"calls\": 155, \"std\": \"75.99us\", \"mean\": \"196.10us\"}, \"reward.parsing.score\": {\"calls\": 155, \"std\": \"4.64ms\", \"mean\": \"4.59ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 92, \"std\": \"29.45us\", \"mean\": \"90.89us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"4.09ms\", \"mean\": \"2.76ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"32.16us\", \"mean\": \"104.13us\"}} counters={\"agent_conn.reward\": {\"calls\": 70, \"mean\": 2.7999999999999994, \"std\": 2.3871637406423725}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5166666666666669, \"std\": 0.5005571032368754}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 63, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 84, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 155, \"mean\": 3703.7999999999993, \"std\": 49.28770558747862, \"value\": 3838.0}} (export_time=104.67us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:16,944] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.1 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=170607.8 vnc_pixels_ps[total]=136332.9 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:17,740] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 31 reward messages to agent: reward=159.0 reward_min=3.0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 8403, 'rewarder.vnc.updates.pixels': 5718, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2084 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:18,771] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=252.0 reward_min=7.0 reward_max=9.0 done=False info={'rewarder.vnc.updates.bytes': 7733, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:21,944] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 160, \"std\": \"32.29us\", \"mean\": \"99.79us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 84, \"std\": \"1.05ms\", \"mean\": \"9.60ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"16.41us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 160, \"std\": \"22.33us\", \"mean\": \"53.32us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"4.54ms\", \"mean\": \"13.16ms\"}, \"reward.parsing.gameover\": {\"calls\": 160, \"std\": \"88.49us\", \"mean\": \"221.18us\"}, \"reward.parsing.score\": {\"calls\": 160, \"std\": \"4.83ms\", \"mean\": \"5.41ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 125, \"std\": \"33.88us\", \"mean\": \"95.74us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"4.49ms\", \"mean\": \"3.28ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"31.61us\", \"mean\": \"99.48us\"}} counters={\"agent_conn.reward\": {\"calls\": 87, \"mean\": 6.9885057471264345, \"std\": 1.5438870671552958}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5315614617940196, \"std\": 0.4998338594405038}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 35, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 76, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 160, \"mean\": 4260.768750000001, \"std\": 211.21660829837512, \"value\": 4446.0}} (export_time=247.24us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:21,945] [INFO:universe.rewarder.remote] [Rewarder] Over past 3.17s, sent 32 reward messages to agent: reward=217.0 reward_min=0 reward_max=10.0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0, 'rewarder.profile': '<2083 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:21,961] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.9 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=226717.6 vnc_pixels_ps[total]=162454.9 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:22,971] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 16 reward messages to agent: reward=16.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 5196, 'rewarder.vnc.updates.pixels': 3200, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:24,003] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=83.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 11589, 'rewarder.vnc.updates.pixels': 3850, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:26,869] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.87s, sent 7 reward messages to agent: reward=22.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 5303, 'rewarder.vnc.updates.pixels': 4542, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:26,970] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 156, \"std\": \"60.57us\", \"mean\": \"121.51us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 59, \"std\": \"875.97us\", \"mean\": \"9.23ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"15.88us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 156, \"std\": \"29.94us\", \"mean\": \"61.97us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"3.79ms\", \"mean\": \"14.02ms\"}, \"reward.parsing.gameover\": {\"calls\": 156, \"std\": \"113.79us\", \"mean\": \"248.38us\"}, \"reward.parsing.score\": {\"calls\": 156, \"std\": \"4.47ms\", \"mean\": \"3.91ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 116, \"std\": \"44.12us\", \"mean\": \"110.28us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"3.80ms\", \"mean\": \"2.49ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"66.17us\", \"mean\": \"119.12us\"}} counters={\"agent_conn.reward\": {\"calls\": 57, \"mean\": 2.157894736842105, \"std\": 1.1617332138448895}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5182724252491692, \"std\": 0.5004980907760107}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 40, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 97, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 156, \"mean\": 4526.1858974358975, \"std\": 49.4520692658386, \"value\": 4569.0}} (export_time=127.79us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:26,977] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.1 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=183247.7 vnc_pixels_ps[total]=142994.6 reward_lag=None rewarder_message_lag=None fps=60.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:27,902] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=50.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 10478, 'rewarder.vnc.updates.pixels': 5880, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2085 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:28,937] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=125.0 reward_min=3.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 7796, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:31,770] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.83s, sent 6 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 8784, 'rewarder.vnc.updates.pixels': 5334, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:31,973] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 154, \"std\": \"26.54us\", \"mean\": \"99.32us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 71, \"std\": \"1.15ms\", \"mean\": \"9.24ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"17.00us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 154, \"std\": \"22.60us\", \"mean\": \"54.06us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"4.12ms\", \"mean\": \"13.70ms\"}, \"reward.parsing.gameover\": {\"calls\": 154, \"std\": \"83.72us\", \"mean\": \"214.00us\"}, \"reward.parsing.score\": {\"calls\": 154, \"std\": \"4.66ms\", \"mean\": \"4.63ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 113, \"std\": \"32.99us\", \"mean\": \"93.47us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"4.11ms\", \"mean\": \"2.78ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"34.34us\", \"mean\": \"105.42us\"}} counters={\"agent_conn.reward\": {\"calls\": 71, \"mean\": 2.7887323943661975, \"std\": 1.5484138700872163}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5133333333333331, \"std\": 0.5006573159847485}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 41, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 83, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 154, \"mean\": 4707.441558441555, \"std\": 71.45435830256436, \"value\": 4767.0}} (export_time=103.47us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:31,994] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=30.7 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=213728.1 vnc_pixels_ps[total]=157842.1 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:32,771] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 31 reward messages to agent: reward=50.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 10185, 'rewarder.vnc.updates.pixels': 3376, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2077 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:33,806] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 30 reward messages to agent: reward=123.0 reward_min=3.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 7999, 'rewarder.vnc.updates.pixels': 5334, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:36,977] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 157, \"std\": \"29.14us\", \"mean\": \"97.72us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 82, \"std\": \"1.20ms\", \"mean\": \"10.06ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"15.65us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 157, \"std\": \"24.79us\", \"mean\": \"55.57us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"4.77ms\", \"mean\": \"13.13ms\"}, \"reward.parsing.gameover\": {\"calls\": 157, \"std\": \"76.63us\", \"mean\": \"204.81us\"}, \"reward.parsing.score\": {\"calls\": 157, \"std\": \"5.10ms\", \"mean\": \"5.62ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 106, \"std\": \"26.23us\", \"mean\": \"91.87us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"4.71ms\", \"mean\": \"3.32ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"27.71us\", \"mean\": \"96.00us\"}} counters={\"agent_conn.reward\": {\"calls\": 85, \"mean\": 4.270588235294118, \"std\": 2.1458056278976176}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5215946843853817, \"std\": 0.5003653150498977}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 51, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 75, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 157, \"mean\": 4996.694267515928, \"std\": 141.1360346879572, \"value\": 5130.0}} (export_time=151.40us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:36,977] [INFO:universe.rewarder.remote] [Rewarder] Over past 3.17s, sent 30 reward messages to agent: reward=195.0 reward_min=0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0, 'rewarder.profile': '<2084 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:37,011] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=191765.7 vnc_pixels_ps[total]=161403.4 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:38,038] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 20 reward messages to agent: reward=26.0 reward_min=1.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 9271, 'rewarder.vnc.updates.pixels': 5496, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:41,002] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.96s, sent 12 reward messages to agent: reward=27.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 7246, 'rewarder.vnc.updates.pixels': 4054, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:41,977] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 160, \"std\": \"27.08us\", \"mean\": \"102.43us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 62, \"std\": \"1.24ms\", \"mean\": \"9.71ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"13.86us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 160, \"std\": \"23.32us\", \"mean\": \"57.63us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"4.18ms\", \"mean\": \"13.88ms\"}, \"reward.parsing.gameover\": {\"calls\": 160, \"std\": \"73.63us\", \"mean\": \"192.22us\"}, \"reward.parsing.score\": {\"calls\": 160, \"std\": \"4.82ms\", \"mean\": \"4.14ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 86, \"std\": \"31.73us\", \"mean\": \"94.53us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"4.15ms\", \"mean\": \"2.60ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"29.65us\", \"mean\": \"100.96us\"}} counters={\"agent_conn.reward\": {\"calls\": 61, \"mean\": 1.6229508196721314, \"std\": 0.7109602526684011}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5333333333333335, \"std\": 0.49972121547874504}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 74, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 98, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 160, \"mean\": 5174.618750000004, \"std\": 22.99928133276836, \"value\": 5226.0}} (export_time=165.94us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:42,003] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=49.0 reward_min=0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 11630, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2080 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:42,028] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.9 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=208058.5 vnc_pixels_ps[total]=147928.0 reward_lag=None rewarder_message_lag=None fps=60.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:43,039] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 31 reward messages to agent: reward=10136.0 reward_min=3.0 reward_max=10005.0 done=False info={'rewarder.vnc.updates.bytes': 8273, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:44,102] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 31 reward messages to agent: reward=245.0 reward_min=6.0 reward_max=17.0 done=False info={'rewarder.vnc.updates.bytes': 8273, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:45,104] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=320.0 reward_min=9.0 reward_max=12.0 done=False info={'rewarder.vnc.updates.bytes': 9593, 'rewarder.vnc.updates.pixels': 14190, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:46,106] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=392.0 reward_min=12.0 reward_max=14.0 done=False info={'rewarder.vnc.updates.bytes': 8273, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:47,002] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 162, \"std\": \"16.06us\", \"mean\": \"83.48us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 151, \"std\": \"1.14ms\", \"mean\": \"9.57ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"12.72us\", \"mean\": \"16.75ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 162, \"std\": \"11.24us\", \"mean\": \"45.17us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"5.23ms\", \"mean\": \"11.14ms\"}, \"reward.parsing.gameover\": {\"calls\": 162, \"std\": \"49.82us\", \"mean\": \"200.43us\"}, \"reward.parsing.score\": {\"calls\": 162, \"std\": \"2.68ms\", \"mean\": \"9.28ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 145, \"std\": \"17.51us\", \"mean\": \"83.02us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"5.18ms\", \"mean\": \"5.37ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"25.69us\", \"mean\": \"91.15us\"}} counters={\"agent_conn.reward\": {\"calls\": 150, \"mean\": 76.52666666666663, \"std\": 816.1065670697166}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5382059800664448, \"std\": 0.499368371809949}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 17, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 11, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 161, \"mean\": 14503.186335403721, \"std\": 3611.84013732088, \"value\": 16708.0}} (export_time=153.54us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:47,044] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=32.3 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=312399.5 vnc_pixels_ps[total]=178670.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:47,138] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=463.0 reward_min=14.0 reward_max=16.0 done=False info={'rewarder.vnc.updates.bytes': 8399, 'rewarder.vnc.updates.pixels': 5178, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2086 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:48,170] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=1379.0 reward_min=3.0 reward_max=1016.0 done=False info={'rewarder.vnc.updates.bytes': 8752, 'rewarder.vnc.updates.pixels': 5718, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:49,204] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=1349.0 reward_min=5.0 reward_max=1008.0 done=False info={'rewarder.vnc.updates.bytes': 14320, 'rewarder.vnc.updates.pixels': 4760, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:50,238] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=462.0 reward_min=13.0 reward_max=16.0 done=False info={'rewarder.vnc.updates.bytes': 11630, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:51,271] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=452.0 reward_min=13.0 reward_max=16.0 done=False info={'rewarder.vnc.updates.bytes': 9702, 'rewarder.vnc.updates.pixels': 6054, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:52,010] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 162, \"std\": \"18.03us\", \"mean\": \"85.62us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 136, \"std\": \"1.07ms\", \"mean\": \"9.40ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"13.93us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 162, \"std\": \"8.67us\", \"mean\": \"43.20us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"5.05ms\", \"mean\": \"11.65ms\"}, \"reward.parsing.gameover\": {\"calls\": 162, \"std\": \"47.47us\", \"mean\": \"195.18us\"}, \"reward.parsing.score\": {\"calls\": 162, \"std\": \"3.61ms\", \"mean\": \"8.23ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 147, \"std\": \"16.24us\", \"mean\": \"80.15us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"5.00ms\", \"mean\": \"4.80ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"22.50us\", \"mean\": \"91.96us\"}} counters={\"agent_conn.reward\": {\"calls\": 139, \"mean\": 27.92805755395684, \"std\": 119.37231006379862}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5382059800664455, \"std\": 0.4993683718099491}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 15, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 26, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 162, \"mean\": 19325.561728395052, \"std\": 1100.6705349925182, \"value\": 20590.0}} (export_time=163.32us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:52,061] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=32.3 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=287403.1 vnc_pixels_ps[total]=195907.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:54,270] [INFO:universe.rewarder.remote] [Rewarder] Over past 3.00s, sent 12 reward messages to agent: reward=161.0 reward_min=0 reward_max=16.0 done=False info={'rewarder.vnc.updates.bytes': 7927, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2086 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:55,271] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=49.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 4204, 'rewarder.vnc.updates.pixels': 3200, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:57,011] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 153, \"std\": \"32.46us\", \"mean\": \"111.32us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 53, \"std\": \"838.15us\", \"mean\": \"9.65ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"17.90us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 153, \"std\": \"23.63us\", \"mean\": \"60.11us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"3.85ms\", \"mean\": \"14.13ms\"}, \"reward.parsing.gameover\": {\"calls\": 153, \"std\": \"97.48us\", \"mean\": \"228.17us\"}, \"reward.parsing.score\": {\"calls\": 153, \"std\": \"4.61ms\", \"mean\": \"3.74ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 100, \"std\": \"37.78us\", \"mean\": \"106.41us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"3.83ms\", \"mean\": \"2.34ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"35.10us\", \"mean\": \"111.52us\"}} counters={\"agent_conn.reward\": {\"calls\": 54, \"mean\": 2.4074074074074074, \"std\": 1.2516934720084718}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5099999999999995, \"std\": 0.500735245367427}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 53, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 100, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 153, \"mean\": 20632.908496732038, \"std\": 53.22610156546085, \"value\": 20720.0}} (export_time=127.08us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:57,011] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.74s, sent 23 reward messages to agent: reward=80.0 reward_min=0.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2091 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:57,078] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=30.7 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=155194.5 vnc_pixels_ps[total]=136832.9 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:58,634] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.62s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 7733, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:22:59,637] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 31 reward messages to agent: reward=50.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 7733, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:02,028] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 156, \"std\": \"30.08us\", \"mean\": \"108.81us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 54, \"std\": \"1.07ms\", \"mean\": \"9.19ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"15.50us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 156, \"std\": \"18.81us\", \"mean\": \"57.00us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"3.71ms\", \"mean\": \"14.20ms\"}, \"reward.parsing.gameover\": {\"calls\": 156, \"std\": \"87.50us\", \"mean\": \"219.26us\"}, \"reward.parsing.score\": {\"calls\": 156, \"std\": \"4.41ms\", \"mean\": \"3.57ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 103, \"std\": \"32.20us\", \"mean\": \"102.27us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"3.68ms\", \"mean\": \"2.28ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"36.48us\", \"mean\": \"109.16us\"}} counters={\"agent_conn.reward\": {\"calls\": 55, \"mean\": 2.3818181818181814, \"std\": 1.2544868965985154}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5182724252491693, \"std\": 0.5004980907760108}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 53, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 102, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 156, \"mean\": 20776.788461538454, \"std\": 57.458316966179474, \"value\": 20851.0}} (export_time=270.37us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:02,030] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.39s, sent 23 reward messages to agent: reward=80.0 reward_min=0.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 3401, 'rewarder.vnc.updates.pixels': 1660, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2089 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:02,095] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=30.9 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=140128.0 vnc_pixels_ps[total]=131564.2 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:03,038] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 8003, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:04,069] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=51.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 14320, 'rewarder.vnc.updates.pixels': 4760, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:07,044] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 157, \"std\": \"28.29us\", \"mean\": \"101.03us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 60, \"std\": \"1.13ms\", \"mean\": \"9.28ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"14.00us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 157, \"std\": \"18.48us\", \"mean\": \"52.90us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"3.89ms\", \"mean\": \"14.01ms\"}, \"reward.parsing.gameover\": {\"calls\": 157, \"std\": \"80.23us\", \"mean\": \"216.66us\"}, \"reward.parsing.score\": {\"calls\": 157, \"std\": \"4.55ms\", \"mean\": \"3.91ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 116, \"std\": \"30.28us\", \"mean\": \"97.72us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"3.86ms\", \"mean\": \"2.46ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"35.56us\", \"mean\": \"108.62us\"}} counters={\"agent_conn.reward\": {\"calls\": 61, \"mean\": 2.6885245901639343, \"std\": 1.4322590967484128}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5215946843853819, \"std\": 0.5003653150498976}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 41, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 97, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 157, \"mean\": 20938.592356687892, \"std\": 71.65622661709844, \"value\": 21015.0}} (export_time=172.62us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:07,044] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.98s, sent 29 reward messages to agent: reward=112.0 reward_min=0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0, 'rewarder.profile': '<2084 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:07,111] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=203252.3 vnc_pixels_ps[total]=144529.2 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:08,068] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 14 reward messages to agent: reward=14.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 8407, 'rewarder.vnc.updates.pixels': 12630, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:09,070] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=62.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 12213, 'rewarder.vnc.updates.pixels': 4058, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *27 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *28 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *29 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *30 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *31 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *32 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *33 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *34 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *35 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:23:11 [info] 65#65: *36 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:12,061] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 161, \"std\": \"31.73us\", \"mean\": \"104.04us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 58, \"std\": \"1.22ms\", \"mean\": \"9.06ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"13.62us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 161, \"std\": \"22.72us\", \"mean\": \"58.05us\"}, \"rewarder_protocol.latency.rtt.skew_unadjusted\": {\"calls\": 10, \"std\": \"526.91us\", \"mean\": \"1.05ms\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"3.74ms\", \"mean\": \"14.09ms\"}, \"reward.parsing.gameover\": {\"calls\": 161, \"std\": \"81.65us\", \"mean\": \"227.93us\"}, \"reward.parsing.score\": {\"calls\": 161, \"std\": \"4.39ms\", \"mean\": \"3.64ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 115, \"std\": \"31.04us\", \"mean\": \"101.15us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"3.72ms\", \"mean\": \"2.39ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"91.94us\", \"mean\": \"118.08us\"}} counters={\"agent_conn.reward\": {\"calls\": 59, \"mean\": 2.033898305084746, \"std\": 1.1290231006908713}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5348837209302327, \"std\": 0.4996122527528476}, \"rewarder_protocol.messages\": {\"calls\": 10, \"mean\": 1.0, \"std\": 0.0}, \"rewarder_protocol.messages.v0.control.ping\": {\"calls\": 10, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 46, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 103, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 161, \"mean\": 21092.515527950316, \"std\": 50.18741196630631, \"value\": 21135.0}} (export_time=225.31us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:12,062] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.99s, sent 15 reward messages to agent: reward=44.0 reward_min=0.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 44, 'rewarder.vnc.updates.pixels': 1660, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2487 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:12,127] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.9 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=164149.6 vnc_pixels_ps[total]=142452.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:13,070] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 28 reward messages to agent: reward=44.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 8273, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:14,070] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 29 reward messages to agent: reward=115.0 reward_min=3.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 7733, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:15,071] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=207.0 reward_min=5.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 8687, 'rewarder.vnc.updates.pixels': 5718, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:16,077] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 31 reward messages to agent: reward=307.0 reward_min=8.0 reward_max=11.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:17,077] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 162, \"std\": \"14.03us\", \"mean\": \"80.95us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 145, \"std\": \"1.09ms\", \"mean\": \"9.34ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"16.85us\", \"mean\": \"16.75ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 162, \"std\": \"9.57us\", \"mean\": \"42.83us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"5.04ms\", \"mean\": \"11.44ms\"}, \"reward.parsing.gameover\": {\"calls\": 162, \"std\": \"37.74us\", \"mean\": \"178.61us\"}, \"reward.parsing.score\": {\"calls\": 162, \"std\": \"3.07ms\", \"mean\": \"8.69ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 136, \"std\": \"8.88us\", \"mean\": \"74.74us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"5.01ms\", \"mean\": \"5.04ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"25.60us\", \"mean\": \"95.44us\"}} counters={\"agent_conn.reward\": {\"calls\": 145, \"mean\": 6.7862068965517235, \"std\": 3.928503756196766}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5382059800664455, \"std\": 0.49936837180994903}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 26, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 17, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 161, \"mean\": 21477.465838509306, \"std\": 308.19348693345626, \"value\": 22119.0}} (export_time=161.89us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:17,077] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 27 reward messages to agent: reward=311.0 reward_min=0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0, 'rewarder.profile': '<2083 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:17,144] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=32.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=328490.7 vnc_pixels_ps[total]=193249.5 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:19,569] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.49s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 8821, 'rewarder.vnc.updates.pixels': 5718, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:20,571] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 29 reward messages to agent: reward=47.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 17701, 'rewarder.vnc.updates.pixels': 5880, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:21,594] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 31 reward messages to agent: reward=128.0 reward_min=2.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 11487, 'rewarder.vnc.updates.pixels': 3822, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:22,094] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 152, \"std\": \"26.87us\", \"mean\": \"98.53us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 61, \"std\": \"1.11ms\", \"mean\": \"9.00ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"11.93us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 152, \"std\": \"22.58us\", \"mean\": \"54.98us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"3.81ms\", \"mean\": \"14.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 152, \"std\": \"74.24us\", \"mean\": \"207.73us\"}, \"reward.parsing.score\": {\"calls\": 152, \"std\": \"4.46ms\", \"mean\": \"3.97ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 105, \"std\": \"29.09us\", \"mean\": \"91.66us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"3.78ms\", \"mean\": \"2.41ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"30.85us\", \"mean\": \"105.40us\"}} counters={\"agent_conn.reward\": {\"calls\": 62, \"mean\": 2.8387096774193545, \"std\": 1.559671554191456}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.504983388704319, \"std\": 0.5008077639072818}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 47, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 91, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 152, \"mean\": 22164.04605263158, \"std\": 63.39546214104555, \"value\": 22295.0}} (export_time=183.58us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:22,161] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=30.3 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=244450.5 vnc_pixels_ps[total]=167952.9 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:24,235] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.64s, sent 2 reward messages to agent: reward=1.0 reward_min=0.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 7232, 'rewarder.vnc.updates.pixels': 5334, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2080 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:25,270] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=53.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:27,095] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 152, \"std\": \"30.32us\", \"mean\": \"107.62us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 52, \"std\": \"942.60us\", \"mean\": \"9.32ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"30.64us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 152, \"std\": \"21.96us\", \"mean\": \"56.49us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"3.69ms\", \"mean\": \"14.24ms\"}, \"reward.parsing.gameover\": {\"calls\": 152, \"std\": \"85.84us\", \"mean\": \"227.47us\"}, \"reward.parsing.score\": {\"calls\": 152, \"std\": \"4.44ms\", \"mean\": \"3.57ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 110, \"std\": \"34.84us\", \"mean\": \"101.69us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"3.66ms\", \"mean\": \"2.24ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"38.18us\", \"mean\": \"111.73us\"}} counters={\"agent_conn.reward\": {\"calls\": 53, \"mean\": 2.4905660377358494, \"std\": 1.3390961577099731}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5066666666666666, \"std\": 0.5007909017876288}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 42, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 100, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 152, \"mean\": 22338.77631578948, \"std\": 54.193081028574596, \"value\": 22427.0}} (export_time=242.47us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:27,096] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.83s, sent 21 reward messages to agent: reward=78.0 reward_min=0.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 22, 'rewarder.vnc.updates.pixels': 1280, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2093 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:27,178] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=30.3 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=241097.8 vnc_pixels_ps[total]=161457.3 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:28,569] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.47s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 8403, 'rewarder.vnc.updates.pixels': 5718, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:29,603] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=53.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:32,111] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 158, \"std\": \"32.13us\", \"mean\": \"107.85us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 52, \"std\": \"969.55us\", \"mean\": \"9.07ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"16.34us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 158, \"std\": \"26.98us\", \"mean\": \"59.06us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"3.58ms\", \"mean\": \"14.28ms\"}, \"reward.parsing.gameover\": {\"calls\": 158, \"std\": \"87.47us\", \"mean\": \"215.37us\"}, \"reward.parsing.score\": {\"calls\": 158, \"std\": \"4.28ms\", \"mean\": \"3.37ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 102, \"std\": \"35.85us\", \"mean\": \"95.94us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"3.56ms\", \"mean\": \"2.20ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"35.89us\", \"mean\": \"113.67us\"}} counters={\"agent_conn.reward\": {\"calls\": 53, \"mean\": 2.4716981132075473, \"std\": 1.3388251678948522}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5249169435215942, \"std\": 0.5002103654916532}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 56, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 106, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 158, \"mean\": 22489.765822784815, \"std\": 58.49015026464583, \"value\": 22558.0}} (export_time=196.46us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:32,112] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.51s, sent 21 reward messages to agent: reward=77.0 reward_min=0.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2084 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:32,194] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=260771.9 vnc_pixels_ps[total]=169184.4 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:33,137] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 8 reward messages to agent: reward=8.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 7246, 'rewarder.vnc.updates.pixels': 4054, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:34,170] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=70.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 8065, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:37,127] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 158, \"std\": \"26.19us\", \"mean\": \"98.87us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 61, \"std\": \"1.09ms\", \"mean\": \"8.90ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"12.59us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 158, \"std\": \"22.26us\", \"mean\": \"52.18us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"3.75ms\", \"mean\": \"14.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 158, \"std\": \"74.59us\", \"mean\": \"210.13us\"}, \"reward.parsing.score\": {\"calls\": 158, \"std\": \"4.38ms\", \"mean\": \"3.79ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 117, \"std\": \"32.65us\", \"mean\": \"93.42us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"3.73ms\", \"mean\": \"2.41ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"31.28us\", \"mean\": \"110.19us\"}} counters={\"agent_conn.reward\": {\"calls\": 62, \"mean\": 2.8548387096774186, \"std\": 1.5873141992061894}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5249169435215942, \"std\": 0.5002103654916533}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 41, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 97, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 158, \"mean\": 22660.569620253158, \"std\": 76.40975419513332, \"value\": 22735.0}} (export_time=129.70us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:37,128] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.96s, sent 23 reward messages to agent: reward=99.0 reward_min=0.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 7619, 'rewarder.vnc.updates.pixels': 5334, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2080 bytes>'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:37,211] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=269266.3 vnc_pixels_ps[total]=173435.3 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:38,136] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 19 reward messages to agent: reward=24.0 reward_min=1.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 17701, 'rewarder.vnc.updates.pixels': 5880, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:41,504] [INFO:universe.rewarder.remote] [Rewarder] Over past 3.37s, sent 23 reward messages to agent: reward=58.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 8701, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:42,138] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 158, \"std\": \"27.99us\", \"mean\": \"101.13us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 63, \"std\": \"1.01ms\", \"mean\": \"9.18ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"11.88us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 158, \"std\": \"18.57us\", \"mean\": \"50.20us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"3.90ms\", \"mean\": \"13.98ms\"}, \"reward.parsing.gameover\": {\"calls\": 158, \"std\": \"76.64us\", \"mean\": \"222.50us\"}, \"reward.parsing.score\": {\"calls\": 158, \"std\": \"4.54ms\", \"mean\": \"4.03ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 133, \"std\": \"29.12us\", \"mean\": \"95.11us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"3.90ms\", \"mean\": \"2.54ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"29.52us\", \"mean\": \"104.96us\"}} counters={\"agent_conn.reward\": {\"calls\": 62, \"mean\": 1.7258064516129035, \"std\": 0.8331129622245396}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5266666666666661, \"std\": 0.5001226159575232}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 25, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 95, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 158, \"mean\": 22797.53164556961, \"std\": 32.17857634543923, \"value\": 22842.0}} (export_time=119.45us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:42,227] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=347974.0 vnc_pixels_ps[total]=196180.2 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:42,504] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 31 reward messages to agent: reward=52.0 reward_min=0.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 17214, 'rewarder.vnc.updates.pixels': 5718, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2082 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:43,536] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=142.0 reward_min=3.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:44,537] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=241.0 reward_min=6.0 reward_max=10.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:45,537] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=335.0 reward_min=10.0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:46,537] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=413.0 reward_min=13.0 reward_max=15.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:47,144] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 161, \"std\": \"16.55us\", \"mean\": \"80.93us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 150, \"std\": \"1.04ms\", \"mean\": \"9.37ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"9.36us\", \"mean\": \"16.75ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 161, \"std\": \"11.49us\", \"mean\": \"43.26us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"5.07ms\", \"mean\": \"11.20ms\"}, \"reward.parsing.gameover\": {\"calls\": 161, \"std\": \"45.54us\", \"mean\": \"196.34us\"}, \"reward.parsing.score\": {\"calls\": 161, \"std\": \"2.58ms\", \"mean\": \"9.08ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 150, \"std\": \"17.98us\", \"mean\": \"78.04us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"5.03ms\", \"mean\": \"5.24ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"25.04us\", \"mean\": \"94.37us\"}} counters={\"agent_conn.reward\": {\"calls\": 151, \"mean\": 9.48344370860927, \"std\": 4.301711759498786}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5348837209302325, \"std\": 0.49961225275284743}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 11, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 11, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 161, \"mean\": 23373.00621118012, \"std\": 427.66250561766606, \"value\": 24258.0}} (export_time=166.65us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:47,244] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=32.3 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=545745.6 vnc_pixels_ps[total]=229001.5 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:47,569] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 32 reward messages to agent: reward=1475.0 reward_min=0 reward_max=1016.0 done=False info={'rewarder.vnc.updates.bytes': 8725, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2086 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:48,570] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=1424.0 reward_min=13.0 reward_max=1014.0 done=False info={'rewarder.vnc.updates.bytes': 17701, 'rewarder.vnc.updates.pixels': 5880, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:49,570] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=2423.0 reward_min=12.0 reward_max=1016.0 done=False info={'rewarder.vnc.updates.bytes': 7663, 'rewarder.vnc.updates.pixels': 5514, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:52,144] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 159, \"std\": \"33.82us\", \"mean\": \"102.39us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 76, \"std\": \"922.14us\", \"mean\": \"9.10ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"12.36us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 159, \"std\": \"24.64us\", \"mean\": \"52.96us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"4.15ms\", \"mean\": \"13.54ms\"}, \"reward.parsing.gameover\": {\"calls\": 159, \"std\": \"80.69us\", \"mean\": \"223.23us\"}, \"reward.parsing.score\": {\"calls\": 159, \"std\": \"4.57ms\", \"mean\": \"4.73ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 129, \"std\": \"31.62us\", \"mean\": \"94.56us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"4.12ms\", \"mean\": \"2.94ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"34.87us\", \"mean\": \"109.20us\"}} counters={\"agent_conn.reward\": {\"calls\": 79, \"mean\": 64.54430379746834, \"std\": 220.96958820037878}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5299999999999997, \"std\": 0.4999331058930715}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 30, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 83, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 159, \"mean\": 28030.66666666667, \"std\": 1650.381331718336, \"value\": 29373.0}} (export_time=140.43us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:52,144] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.57s, sent 6 reward messages to agent: reward=51.0 reward_min=0 reward_max=11.0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0, 'rewarder.profile': '<2087 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:52,261] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.7 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=244526.7 vnc_pixels_ps[total]=160288.6 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:53,170] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 25 reward messages to agent: reward=35.0 reward_min=1.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:54,205] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=114.0 reward_min=2.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 11589, 'rewarder.vnc.updates.pixels': 3850, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:56,937] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.73s, sent 4 reward messages to agent: reward=16.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 8702, 'rewarder.vnc.updates.pixels': 5718, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:57,144] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 156, \"std\": \"27.22us\", \"mean\": \"97.78us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 68, \"std\": \"1.07ms\", \"mean\": \"9.13ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"12.87us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 156, \"std\": \"20.89us\", \"mean\": \"52.80us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"4.01ms\", \"mean\": \"13.81ms\"}, \"reward.parsing.gameover\": {\"calls\": 156, \"std\": \"76.05us\", \"mean\": \"206.50us\"}, \"reward.parsing.score\": {\"calls\": 156, \"std\": \"4.57ms\", \"mean\": \"4.34ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 111, \"std\": \"30.00us\", \"mean\": \"90.81us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"3.99ms\", \"mean\": \"2.67ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"34.00us\", \"mean\": \"109.93us\"}} counters={\"agent_conn.reward\": {\"calls\": 67, \"mean\": 2.55223880597015, \"std\": 1.4902062302830137}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5233333333333331, \"std\": 0.5069307825365965}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 45, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 88, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 156, \"mean\": 29487.269230769234, \"std\": 64.4578728296804, \"value\": 29543.0}} (export_time=150.68us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:57,277] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.7 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=261295.6 vnc_pixels_ps[total]=174738.8 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:57,970] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 32 reward messages to agent: reward=54.0 reward_min=0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 8041, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2080 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:23:58,977] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 30 reward messages to agent: reward=80.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:00,003] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 30 reward messages to agent: reward=182.0 reward_min=5.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 8731, 'rewarder.vnc.updates.pixels': 4376, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:01,003] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=278.0 reward_min=8.0 reward_max=11.0 done=False info={'rewarder.vnc.updates.bytes': 11589, 'rewarder.vnc.updates.pixels': 3850, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:02,144] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 155, \"std\": \"25.44us\", \"mean\": \"86.66us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 115, \"std\": \"1.13ms\", \"mean\": \"9.06ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"13.22us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 155, \"std\": \"13.94us\", \"mean\": \"44.31us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"4.69ms\", \"mean\": \"12.43ms\"}, \"reward.parsing.gameover\": {\"calls\": 155, \"std\": \"52.54us\", \"mean\": \"194.39us\"}, \"reward.parsing.score\": {\"calls\": 155, \"std\": \"4.08ms\", \"mean\": \"7.07ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 134, \"std\": \"23.86us\", \"mean\": \"81.48us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"4.67ms\", \"mean\": \"4.05ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"38.28us\", \"mean\": \"109.59us\"}} counters={\"agent_conn.reward\": {\"calls\": 118, \"mean\": 5.016949152542375, \"std\": 3.1021995253091528}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5166666666666663, \"std\": 0.5005571032368754}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 21, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 40, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 155, \"mean\": 29813.799999999985, \"std\": 218.38279271922102, \"value\": 30136.0}} (export_time=202.89us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:02,144] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.14s, sent 3 reward messages to agent: reward=4.0 reward_min=0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0, 'rewarder.profile': '<2088 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:02,294] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=30.3 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=319534.3 vnc_pixels_ps[total]=183812.1 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:03,704] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.56s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 10324, 'rewarder.vnc.updates.pixels': 3422, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:04,736] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=51.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 8764, 'rewarder.vnc.updates.pixels': 5832, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:05,737] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=127.0 reward_min=3.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 17701, 'rewarder.vnc.updates.pixels': 5880, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:06,772] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=219.0 reward_min=6.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:07,171] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 160, \"std\": \"26.87us\", \"mean\": \"89.80us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 107, \"std\": \"1.00ms\", \"mean\": \"9.20ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"12.50us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 160, \"std\": \"21.05us\", \"mean\": \"50.13us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"4.64ms\", \"mean\": \"12.64ms\"}, \"reward.parsing.gameover\": {\"calls\": 160, \"std\": \"61.86us\", \"mean\": \"195.13us\"}, \"reward.parsing.score\": {\"calls\": 160, \"std\": \"4.39ms\", \"mean\": \"6.51ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 122, \"std\": \"25.83us\", \"mean\": \"82.17us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"4.63ms\", \"mean\": \"3.86ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"34.82us\", \"mean\": \"107.68us\"}} counters={\"agent_conn.reward\": {\"calls\": 105, \"mean\": 4.7333333333333325, \"std\": 2.714868840388579}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.53156146179402, \"std\": 0.4998338594405039}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 38, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 53, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 160, \"mean\": 30255.081250000007, \"std\": 146.5543010908894, \"value\": 30633.0}} (export_time=93.70us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:07,310] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=32.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=307711.5 vnc_pixels_ps[total]=182930.8 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:07,802] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=306.0 reward_min=9.0 reward_max=11.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2085 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:08,837] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=376.0 reward_min=11.0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 17701, 'rewarder.vnc.updates.pixels': 5880, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:09,901] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 32 reward messages to agent: reward=449.0 reward_min=13.0 reward_max=15.0 done=False info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:12,177] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 153, \"std\": \"25.48us\", \"mean\": \"89.13us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 103, \"std\": \"1.01ms\", \"mean\": \"9.08ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"13.10us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 153, \"std\": \"16.39us\", \"mean\": \"46.02us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"4.59ms\", \"mean\": \"12.77ms\"}, \"reward.parsing.gameover\": {\"calls\": 153, \"std\": \"57.87us\", \"mean\": \"193.59us\"}, \"reward.parsing.score\": {\"calls\": 153, \"std\": \"4.34ms\", \"mean\": \"6.46ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 126, \"std\": \"21.63us\", \"mean\": \"80.52us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"4.55ms\", \"mean\": \"3.67ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"30.95us\", \"mean\": \"107.30us\"}} counters={\"agent_conn.reward\": {\"calls\": 106, \"mean\": 12.415094339622643, \"std\": 1.626377362222128}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5083056478405312, \"std\": 0.5007635366131693}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 27, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 50, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 153, \"mean\": 31440.058823529413, \"std\": 452.1398862964717, \"value\": 31949.0}} (export_time=202.42us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:12,178] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.28s, sent 24 reward messages to agent: reward=284.0 reward_min=0 reward_max=15.0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0, 'rewarder.profile': '<2086 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:12,328] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=30.3 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=385876.3 vnc_pixels_ps[total]=193515.6 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:24:13 [info] 65#65: *37 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT clients2.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:13,303] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.12s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 11630, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:14,304] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 29 reward messages to agent: reward=42.0 reward_min=1.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 9743, 'rewarder.vnc.updates.pixels': 6054, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:15,305] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=117.0 reward_min=3.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 9522, 'rewarder.vnc.updates.pixels': 5832, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:16,310] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 31 reward messages to agent: reward=211.0 reward_min=5.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:17,177] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 162, \"std\": \"25.50us\", \"mean\": \"91.46us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 102, \"std\": \"1.04ms\", \"mean\": \"9.20ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"12.12us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 162, \"std\": \"18.05us\", \"mean\": \"49.81us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"4.61ms\", \"mean\": \"12.75ms\"}, \"reward.parsing.gameover\": {\"calls\": 162, \"std\": \"66.11us\", \"mean\": \"200.72us\"}, \"reward.parsing.score\": {\"calls\": 162, \"std\": \"4.51ms\", \"mean\": \"6.15ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 126, \"std\": \"24.37us\", \"mean\": \"86.04us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"4.59ms\", \"mean\": \"3.73ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"31.08us\", \"mean\": \"106.52us\"}} counters={\"agent_conn.reward\": {\"calls\": 103, \"mean\": 4.475728155339806, \"std\": 2.659959679018354}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5399999999999997, \"std\": 0.49923017660270597}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 36, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 60, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 162, \"mean\": 32098.58024691358, \"std\": 164.989618660619, \"value\": 32410.0}} (export_time=175.95us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:17,344] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=32.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=218128.7 vnc_pixels_ps[total]=163131.0 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:19,338] [INFO:universe.rewarder.remote] [Rewarder] Over past 3.03s, sent 13 reward messages to agent: reward=91.0 reward_min=0.0 reward_max=9.0 done=False info={'rewarder.vnc.updates.bytes': 8775, 'rewarder.vnc.updates.pixels': 5334, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2086 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:20,370] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=50.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 8637, 'rewarder.vnc.updates.pixels': 5178, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:22,194] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 152, \"std\": \"26.60us\", \"mean\": \"100.06us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 40, \"std\": \"1.00ms\", \"mean\": \"9.78ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"13.97us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 152, \"std\": \"17.82us\", \"mean\": \"53.21us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"3.47ms\", \"mean\": \"14.59ms\"}, \"reward.parsing.gameover\": {\"calls\": 152, \"std\": \"83.37us\", \"mean\": \"207.32us\"}, \"reward.parsing.score\": {\"calls\": 152, \"std\": \"4.34ms\", \"mean\": \"2.93ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 98, \"std\": \"29.99us\", \"mean\": \"100.32us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"3.45ms\", \"mean\": \"1.90ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"37.67us\", \"mean\": \"116.99us\"}} counters={\"agent_conn.reward\": {\"calls\": 41, \"mean\": 1.8292682926829267, \"std\": 0.8632044666355199}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5049833887043186, \"std\": 0.5008077639072818}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 54, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 112, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 152, \"mean\": 32441.394736842107, \"std\": 33.66196546750569, \"value\": 32485.0}} (export_time=173.81us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:22,195] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.82s, sent 9 reward messages to agent: reward=24.0 reward_min=0.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 44, 'rewarder.vnc.updates.pixels': 1660, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2088 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:22,361] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=30.1 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=118808.2 vnc_pixels_ps[total]=128710.5 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:23,271] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.08s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 11630, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:24,303] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 30 reward messages to agent: reward=49.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 8764, 'rewarder.vnc.updates.pixels': 5832, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:27,003] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.70s, sent 3 reward messages to agent: reward=7.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 7842, 'rewarder.vnc.updates.pixels': 4054, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:27,204] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 160, \"std\": \"31.34us\", \"mean\": \"106.16us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 42, \"std\": \"1.04ms\", \"mean\": \"9.48ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"13.11us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 160, \"std\": \"21.45us\", \"mean\": \"56.78us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"3.39ms\", \"mean\": \"14.53ms\"}, \"reward.parsing.gameover\": {\"calls\": 160, \"std\": \"101.50us\", \"mean\": \"230.98us\"}, \"reward.parsing.score\": {\"calls\": 160, \"std\": \"4.19ms\", \"mean\": \"2.87ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 111, \"std\": \"36.85us\", \"mean\": \"109.32us\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"3.41ms\", \"mean\": \"1.98ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"37.59us\", \"mean\": \"117.15us\"}} counters={\"agent_conn.reward\": {\"calls\": 41, \"mean\": 1.536585365853659, \"std\": 0.7449013684914855}, \"reward.vnc.updates.n\": {\"calls\": 300, \"mean\": 0.5333333333333332, \"std\": 0.4997212154787448}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 49, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 118, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 160, \"mean\": 32521.331249999996, \"std\": 24.876480234709447, \"value\": 32548.0}} (export_time=155.93us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:27,377] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=32.1 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=147515.5 vnc_pixels_ps[total]=140798.6 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:28,010] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 32 reward messages to agent: reward=51.0 reward_min=0.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2087 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:30,737] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.73s, sent 3 reward messages to agent: reward=7.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 11630, 'rewarder.vnc.updates.pixels': 4390, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:31,738] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 30 reward messages to agent: reward=50.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 8694, 'rewarder.vnc.updates.pixels': 5718, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:32,210] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 160, \"std\": \"29.10us\", \"mean\": \"96.51us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 67, \"std\": \"1.13ms\", \"mean\": \"9.27ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"11.82us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 160, \"std\": \"22.80us\", \"mean\": \"54.60us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"4.06ms\", \"mean\": \"13.77ms\"}, \"reward.parsing.gameover\": {\"calls\": 160, \"std\": \"80.50us\", \"mean\": \"209.37us\"}, \"reward.parsing.score\": {\"calls\": 160, \"std\": \"4.63ms\", \"mean\": \"4.24ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 110, \"std\": \"35.41us\", \"mean\": \"96.18us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"4.02ms\", \"mean\": \"2.67ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"35.32us\", \"mean\": \"113.52us\"}} counters={\"agent_conn.reward\": {\"calls\": 70, \"mean\": 2.0000000000000004, \"std\": 0.9630868246861534}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.5315614617940204, \"std\": 0.4998338594405039}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 50, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 93, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 160, \"mean\": 32604.11875, \"std\": 28.3849042781654, \"value\": 32688.0}} (export_time=188.35us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:32,394] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=32.1 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=237507.4 vnc_pixels_ps[total]=169361.8 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:34,736] [INFO:universe.rewarder.remote] [Rewarder] Over past 3.00s, sent 13 reward messages to agent: reward=39.0 reward_min=0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 14507, 'rewarder.vnc.updates.pixels': 14176, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2083 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:35,754] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 30 reward messages to agent: reward=49.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 8403, 'rewarder.vnc.updates.pixels': 5718, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:36,770] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 31 reward messages to agent: reward=130.0 reward_min=3.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 8764, 'rewarder.vnc.updates.pixels': 5832, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:37,235] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 159, \"std\": \"32.44us\", \"mean\": \"96.68us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 78, \"std\": \"1.14ms\", \"mean\": \"9.38ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"13.86us\", \"mean\": \"16.76ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 159, \"std\": \"22.86us\", \"mean\": \"54.44us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"4.31ms\", \"mean\": \"13.46ms\"}, \"reward.parsing.gameover\": {\"calls\": 159, \"std\": \"77.21us\", \"mean\": \"212.03us\"}, \"reward.parsing.score\": {\"calls\": 159, \"std\": \"4.75ms\", \"mean\": \"4.96ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 113, \"std\": \"27.07us\", \"mean\": \"95.16us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"4.30ms\", \"mean\": \"3.04ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"38.08us\", \"mean\": \"114.15us\"}} counters={\"agent_conn.reward\": {\"calls\": 76, \"mean\": 3.447368421052631, \"std\": 1.934905591785503}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.528239202657807, \"std\": 0.5000332214876949}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 46, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 81, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 159, \"mean\": 32735.597484276725, \"std\": 72.92662512319791, \"value\": 32950.0}} (export_time=116.11us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:37,410] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=31.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=184145.9 vnc_pixels_ps[total]=156304.9 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:37,803] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=219.0 reward_min=6.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 7246, 'rewarder.vnc.updates.pixels': 4054, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2078 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:38,836] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=307.0 reward_min=9.0 reward_max=11.0 done=False info={'rewarder.vnc.updates.bytes': 8259, 'rewarder.vnc.updates.pixels': 5670, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:39,927] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.09s, sent 24 reward messages to agent: reward=288.0 reward_min=11.0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 1161, 'rewarder.vnc.updates.pixels': 8576, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,243] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 94, \"std\": \"21.25us\", \"mean\": \"87.73us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 70, \"std\": \"1.05ms\", \"mean\": \"9.11ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"14.70us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 94, \"std\": \"14.01us\", \"mean\": \"44.82us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"4.13ms\", \"mean\": \"13.87ms\"}, \"reward.parsing.gameover\": {\"calls\": 94, \"std\": \"49.72us\", \"mean\": \"186.70us\"}, \"reward.parsing.score\": {\"calls\": 94, \"std\": \"4.08ms\", \"mean\": \"7.12ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 77, \"std\": \"16.93us\", \"mean\": \"79.09us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"4.10ms\", \"mean\": \"2.58ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"36.08us\", \"mean\": \"115.42us\"}} counters={\"agent_conn.reward\": {\"calls\": 73, \"mean\": 10.027397260273972, \"std\": 1.794842942213777}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.31229235880398687, \"std\": 0.4642000943286748}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 17, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 94, \"mean\": 33352.05319148937, \"std\": 245.49525834814497, \"value\": 33682.0}} (export_time=145.20us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,244] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.32s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<2079 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,428] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=17.9 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=124644.2 vnc_pixels_ps[total]=119006.1 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,737] [INFO:universe.utils] [gameover_warnings] Something close to gameover screen detected: warn_n=[True] distance_n=0.0303705. Consider increasing the match_threshold for gameover0 from 0.02 to 0.04\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,835] [INFO:universe.utils] [gameover] Gameover screen detected: distance_n=0.0115158 match_time=159us\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,835] [INFO:gym_controlplane.reward.reward] RESET CAUSE: gameover state reached\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,835] [INFO:root] [Rewarder] Resetting environment since done=True\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,836] [INFO:root] [Rewarder] Triggering a reset on EnvController\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,836] [INFO:root] [EnvStatus] Changing env_state: running (env_id=flashgames.NeonRace-v0) -> resetting (env_id=flashgames.NeonRace-v0) (episode_id: 2->3, fps=60)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,836] [INFO:root] [Rewarder] Blocking until env finishes resetting\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,836] [INFO:root] [EnvController] controlplane.py is resetting the environment\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,837] [INFO:root] [EnvController] Env state: env_id=flashgames.NeonRace-v0 episode_id=3\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,837] [INFO:root] [EnvController] Writing flashgames.NeonRace-v0 to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:42,837] [ERROR:root] Closing server (via subprocess.close()) and all chromes (via pkill chromedriver || :; pkill chrome || :)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:24:42,852] init detected end of child process 435 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:24:42,855] init detected end of child process 450 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:24:42,856] init detected end of child process 656 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:24:42,861] init detected end of child process 678 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:24:42,893] init detected end of child process 691 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:24:43 [info] 64#64: *26 client 127.0.0.1 closed keepalive connection\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:24:43 [info] 65#65: *38 client 127.0.0.1 closed keepalive connection\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [Mon Mar 18 08:24:43 UTC 2019] [/usr/local/bin/sudoable-env-setup] Allowing outbound network traffic to non-private IPs for git-lfs. (Going to fetch files via git lfs.)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [Mon Mar 18 08:24:43 UTC 2019] [/usr/local/bin/sudoable-env-setup] Completion file /usr/local/openai/git-lfs/flashgames.NeonRace-v0 exists; not git-lfs pulling\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:24:43,115] init detected end of child process 438 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:24:43,116] init detected end of child process 446 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:24:43,116] init detected end of child process 447 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [init] [2019-03-18 08:24:43,116] init detected end of child process 449 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [Mon Mar 18 08:24:43 UTC 2019] [/usr/local/bin/sudoable-env-setup] Disabling outbound network traffic for flashgames.NeonRace-v0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:43,262] [INFO:gym_flashgames.launcher] [EnvController] Launching new Chrome process (attempt 0/10)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:43,262] [INFO:root] Replacing selenium_wrapper_server since we currently do it at every episode boundary\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:43,354] [selenium_wrapper_server] Calling webdriver.Chrome()\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:24:44 [info] 65#65: *39 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:24:44 [info] 65#65: *40 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:24:44 [info] 65#65: *41 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:44,660] [selenium_wrapper_server] Call to webdriver.Chrome() completed: 1.31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:44,660] [INFO:gym_flashgames.launcher] [EnvController] Navigating browser to url=http://localhost/flashgames.NeonRace-v0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:44,773] [INFO:root] [EnvController] Running command: /app/universe-envs/controlplane/bin/play_vexpect -e flashgames.NeonRace-v0 -r vnc://127.0.0.1:5900 -d\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:45,335] [play_vexpect] No rewarder addresses were provided, so this env cannot connect to the remote's rewarder channel, and cannot send control messages (e.g. reset)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:45,335] [play_vexpect] Using the golang VNC implementation\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:45,335] [play_vexpect] Using VNCSession arguments: {'encoding': 'zrle', 'compress_level': 0, 'start_timeout': 7, 'fine_quality_level': 50, 'subsample_level': 2}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:45,335] [play_vexpect] Printed stats will ignore clock skew. (This usually makes sense only when the environment and agent are on the same machine.)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:45,341] [play_vexpect] [0] Connecting to environment: vnc://127.0.0.1:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://None/viewer/?password=openai\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:45,341] [play_vexpect] [0] Connecting to environment details: vnc_address=127.0.0.1:5900 vnc_password=openai rewarder_address=None rewarder_password=openai\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m 2019/03/18 08:24:45 I0318 08:24:45.341681 1248 gymvnc.go:417] [0:127.0.0.1:5900] opening connection to VNC server\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Mon Mar 18 08:24:45 2019\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  Connections: accepted: 127.0.0.1::44312\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  SConnection: Client needs protocol version 3.8\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  SConnection: Client requests security type VncAuth(2)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian bgr888\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m 2019/03/18 08:24:45 I0318 08:24:45.344458 1248 gymvnc.go:550] [0:127.0.0.1:5900] connection established\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:45,479] [play_vexpect] Waiting for any of [initialize0] to activate\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m 2019/03/18 08:24:46 I0318 08:24:46.517773 61 gymvnc.go:374] [0:127.0.0.1:5900] update queue max of 60 reached; pausing further updates\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:55,263] [play_vexpect] Applying transition: ClickTransition<initialize0->['initialize1'] x=342 y=487 buttonmask=1> for active state initialize0. (Summary: plausible_states=initialize0 distance_m=0.0 match_time_m=350us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:55,280] [play_vexpect] Waiting for any of [initialize1] to activate (or whether any of [initialize0] are still active)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:59,313] [play_vexpect] Advancing to the next hopeful state (2/2): initialize0\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:24:59,330] [play_vexpect] Advancing to the next hopeful state (1/2): initialize1\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:00,813] [play_vexpect] Applying transition: ClickTransition<initialize1->['initialize2'] x=159 y=217 buttonmask=1> for active state initialize1. (Summary: plausible_states=[initialize1, initialize0] distance_m=[0.0, 1.152208] match_time_m=['280us', '137us'])\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:00,829] [play_vexpect] Waiting for any of [initialize2] to activate (or whether any of [initialize1] are still active)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,247] [play_vexpect] Applying transition: ClickTransition<initialize2->['ready0', 'ready1', 'ready2'] x=107 y=234 buttonmask=1> for active state initialize2. (Summary: plausible_states=[initialize2, initialize1] distance_m=[0.0, 1.1360776] match_time_m=['312us', '134us'])\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,263] [play_vexpect] Waiting for any of [ready0, ready1, ready2] to activate (or whether any of [initialize2] are still active)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,447] [play_vexpect] Applying transition: ClickTransition<ready1->[] x=0 y=0 buttonmask=0> for active state ready1. (Summary: plausible_states=[ready0, ready1, ready2, initialize2] distance_m=[2.5031936, 0.0046253935, 0.0062009618, 1.1523193] match_time_m=['273us', '204us', '150us', '142us'])\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,448] [play_vexpect] Reaching start state: ready1\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,448] [play_vexpect] vexpect macro complete in 16.073236s\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc] Mon Mar 18 08:25:01 2019\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  Connections: closed: 127.0.0.1::44312 (Clean disconnection)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager: Framebuffer updates: 355\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:   ZRLE:\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:     Solid: 3 rects, 1.875 kpixels\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:            90 B (1:83.7333 ratio)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:     Bitmap RLE: 296 rects, 306.671 kpixels\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:                 53.3652 KiB (1:22.5128 ratio)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:     Indexed RLE: 339 rects, 1.23126 Mpixels\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:                  372.967 KiB (1:12.9062 ratio)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:     Full Colour: 16 rects, 510.335 kpixels\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:                  1.46082 MiB (1:1.33278 ratio)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:   Total: 654 rects, 2.05014 Mpixels\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [tigervnc]  EncodeManager:          1.87725 MiB (1:4.17001 ratio)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,662] [INFO:root] [EnvStatus] Changing env_state: resetting (env_id=flashgames.NeonRace-v0) -> running (env_id=flashgames.NeonRace-v0) (episode_id: 3->3, fps=60)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,663] [INFO:root] [Rewarder] Unblocking since env reset finished\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,663] [INFO:root] [Rewarder] Clearing reward_parser state: env_id=flashgames.NeonRace-v0 episode_id=2->3, env_state=running\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,664] [INFO:universe.rewarder.remote] [Rewarder] Over past 19.42s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=True info={'rewarder.vnc.updates.bytes': 18689, 'rewarder.vnc.updates.pixels': 6216, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,664] [INFO:universe.rewarder.remote] [Rewarder] Ending previous episode: episode_reward=33680.0 episode_count=2801 episode_duration=390.46\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m 2019/03/18 08:25:01 I0318 08:25:01.665219 61 gymvnc.go:278] [0:127.0.0.1:5900] resuming updates\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,665] [INFO:universe.wrappers.logger] Stats for the past 19.24s: vnc_updates_ps=0.3 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=4024.8 vnc_pixels_ps[total]=2321.9 reward_lag=None rewarder_message_lag=None fps=1.30\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:01,673] [INFO:universe.pyprofile] [pyprofile] period=19.43s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 8, \"std\": \"35.30us\", \"mean\": \"77.64us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 5, \"std\": \"1.03ms\", \"mean\": \"8.57ms\"}, \"rewarder.frame\": {\"calls\": 36, \"std\": \"1.12ms\", \"mean\": \"16.58ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 8, \"std\": \"23.42us\", \"mean\": \"44.35us\"}, \"rewarder.sleep\": {\"calls\": 35, \"std\": \"2.73ms\", \"mean\": \"15.26ms\"}, \"reward.parsing.gameover\": {\"calls\": 8, \"std\": \"103.35us\", \"mean\": \"233.29us\"}, \"reward.parsing.score\": {\"calls\": 8, \"std\": \"4.46ms\", \"mean\": \"5.61ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 5, \"std\": \"6.86us\", \"mean\": \"71.57us\"}, \"rewarder.compute_reward\": {\"calls\": 36, \"std\": \"3.24ms\", \"mean\": \"1.66ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 36, \"std\": \"33.27us\", \"mean\": \"117.75us\"}} counters={\"agent_conn.done\": {\"calls\": 1, \"mean\": 1.0, \"std\": 0}, \"agent_conn.reward\": {\"calls\": 2, \"mean\": 0.0, \"std\": 0.0}, \"reward.vnc.updates.n\": {\"calls\": 36, \"mean\": 1.8888888888888888, \"std\": 10.14122499723009}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 3, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 3, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 2, \"mean\": 33682.0, \"std\": 0.0, \"value\": 33682.0}} (export_time=126.84us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:03,724] [INFO:gym_controlplane.reward.reward] First score parsed: score=2\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:03,756] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.09s, sent 2 reward messages to agent: reward=2 reward_min=0 reward_max=2 done=False info={'rewarder.vnc.updates.bytes': 7581, 'rewarder.vnc.updates.pixels': 13172, 'rewarder.vnc.updates.n': 2, 'rewarder.profile': '<2075 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:04,790] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 31 reward messages to agent: reward=35.0 reward_min=-2.0 reward_max=2 done=False info={'rewarder.vnc.updates.bytes': 12220, 'rewarder.vnc.updates.pixels': 4054, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:06,681] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=41.7 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=191707.5 vnc_pixels_ps[total]=198633.2 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:06,682] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 145, \"std\": \"51.04us\", \"mean\": \"121.29us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 34, \"std\": \"1.24ms\", \"mean\": \"8.19ms\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"37.97us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 145, \"std\": \"33.96us\", \"mean\": \"72.89us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"2.81ms\", \"mean\": \"14.86ms\"}, \"reward.parsing.gameover\": {\"calls\": 145, \"std\": \"96.10us\", \"mean\": \"229.87us\"}, \"reward.parsing.score\": {\"calls\": 145, \"std\": \"3.53ms\", \"mean\": \"2.29ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 68, \"std\": \"32.93us\", \"mean\": \"104.91us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"2.75ms\", \"mean\": \"1.58ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"66.27us\", \"mean\": \"139.28us\"}} counters={\"agent_conn.reward\": {\"calls\": 35, \"mean\": 1.114285714285714, \"std\": 0.7183080025689688}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.49501661129568103, \"std\": 0.5267590369955493}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 77, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 111, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 93, \"mean\": 34.09677419354837, \"std\": 11.163863010049566, \"value\": 41.0}} (export_time=201.70us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:06,682] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.89s, sent 3 reward messages to agent: reward=2.0 reward_min=0.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 12220, 'rewarder.vnc.updates.pixels': 4054, 'rewarder.vnc.updates.n': 1, 'rewarder.profile': '<2087 bytes>'}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:11,698] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=9.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=26137.8 vnc_pixels_ps[total]=52151.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:11,699] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 46, \"std\": \"43.10us\", \"mean\": \"121.62us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"35.97us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 46, \"std\": \"24.40us\", \"mean\": \"69.77us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"309.56us\", \"mean\": \"16.03ms\"}, \"reward.parsing.gameover\": {\"calls\": 46, \"std\": \"84.42us\", \"mean\": \"189.12us\"}, \"reward.parsing.score\": {\"calls\": 46, \"std\": \"115.12us\", \"mean\": \"394.53us\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 7, \"std\": \"28.72us\", \"mean\": \"101.26us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"287.83us\", \"mean\": \"435.73us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"63.69us\", \"mean\": \"140.10us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.15614617940199338, \"std\": 0.37265323276967266}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 39, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 46, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 46, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=203.37us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:11,700] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1864 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:16,715] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5951.9 vnc_pixels_ps[total]=44160.4 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:16,716] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"23.28us\", \"mean\": \"111.65us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"25.40us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"17.79us\", \"mean\": \"71.31us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"238.68us\", \"mean\": \"16.09ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"35.61us\", \"mean\": \"150.83us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"78.32us\", \"mean\": \"366.28us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"198.44us\", \"mean\": \"386.46us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"53.96us\", \"mean\": \"143.09us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=211.24us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:16,717] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:21,731] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5894.2 vnc_pixels_ps[total]=43715.6 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:21,732] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"26.21us\", \"mean\": \"115.21us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"34.36us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"74.38us\", \"mean\": \"87.04us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"269.02us\", \"mean\": \"16.09ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"83.67us\", \"mean\": \"166.13us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"89.39us\", \"mean\": \"388.74us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"200.84us\", \"mean\": \"373.90us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"48.20us\", \"mean\": \"134.12us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.2660985088079444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=197.65us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:21,732] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:26,748] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6198.0 vnc_pixels_ps[total]=45959.9 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:26,749] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"72.11us\", \"mean\": \"135.88us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"46.90us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"31.97us\", \"mean\": \"82.90us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"292.18us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"74.23us\", \"mean\": \"172.58us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"144.05us\", \"mean\": \"419.60us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"245.26us\", \"mean\": \"385.79us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"52.30us\", \"mean\": \"137.13us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910303, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=287.29us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:26,750] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:31,764] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6034.2 vnc_pixels_ps[total]=44756.5 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:31,765] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"43.07us\", \"mean\": \"125.56us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"31.59us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"28.89us\", \"mean\": \"72.88us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"274.42us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"59.15us\", \"mean\": \"150.79us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"136.63us\", \"mean\": \"405.71us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"208.84us\", \"mean\": \"385.18us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"50.89us\", \"mean\": \"136.02us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=114.92us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:31,765] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1715 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:36,782] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=7257.1 vnc_pixels_ps[total]=43786.1 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:36,783] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"33.05us\", \"mean\": \"118.89us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"26.52us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"32.01us\", \"mean\": \"81.39us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"234.19us\", \"mean\": \"16.10ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"59.46us\", \"mean\": \"173.48us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"127.67us\", \"mean\": \"415.71us\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"66.04us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"236.56us\", \"mean\": \"379.27us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"45.64us\", \"mean\": \"134.29us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910297, \"std\": 0.2713323837260709}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=321.15us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:36,783] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1836 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:41,798] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5989.3 vnc_pixels_ps[total]=44448.1 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:41,798] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"28.83us\", \"mean\": \"110.70us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"59.54us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"21.86us\", \"mean\": \"69.23us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"273.65us\", \"mean\": \"16.12ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"46.88us\", \"mean\": \"144.52us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"96.09us\", \"mean\": \"364.31us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"188.60us\", \"mean\": \"350.99us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"48.93us\", \"mean\": \"128.43us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=157.36us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:41,799] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1722 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:25:44 [info] 65#65: *43 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:25:44 [info] 65#65: *44 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:25:44 [info] 65#65: *45 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:25:44 [info] 65#65: *46 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:46,815] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5792.9 vnc_pixels_ps[total]=43006.6 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:46,816] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"45.80us\", \"mean\": \"120.04us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"28.77us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"22.76us\", \"mean\": \"71.24us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"235.56us\", \"mean\": \"16.09ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"45.59us\", \"mean\": \"148.63us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"102.49us\", \"mean\": \"379.72us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"216.93us\", \"mean\": \"383.65us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"54.84us\", \"mean\": \"139.68us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=295.16us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:46,817] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:51,831] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5952.6 vnc_pixels_ps[total]=44165.1 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:51,831] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"31.92us\", \"mean\": \"128.85us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"37.74us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"28.64us\", \"mean\": \"79.54us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"292.74us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"58.73us\", \"mean\": \"168.72us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"109.40us\", \"mean\": \"426.51us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"218.43us\", \"mean\": \"394.24us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"63.36us\", \"mean\": \"143.52us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=129.22us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:51,832] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:56,848] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5898.1 vnc_pixels_ps[total]=43746.2 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:56,850] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"27.98us\", \"mean\": \"112.03us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"27.13us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"25.48us\", \"mean\": \"72.81us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"226.30us\", \"mean\": \"16.11ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"52.54us\", \"mean\": \"153.18us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"92.81us\", \"mean\": \"375.47us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"240.29us\", \"mean\": \"373.57us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"63.72us\", \"mean\": \"136.53us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910307, \"std\": 0.2713323837260707}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=249.86us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:25:56,851] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0.0 reward_min=0.0 reward_max=0.0 done=False info={'rewarder.vnc.updates.bytes': 1161, 'rewarder.profile': '<1719 bytes>', 'rewarder.vnc.updates.pixels': 8576, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:01,864] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6159.0 vnc_pixels_ps[total]=45697.2 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:01,865] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"38.09us\", \"mean\": \"134.50us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"25.65us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"24.74us\", \"mean\": \"84.77us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"320.58us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"53.79us\", \"mean\": \"180.29us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"112.62us\", \"mean\": \"440.06us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"234.90us\", \"mean\": \"383.93us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"56.14us\", \"mean\": \"137.63us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.26609850880794456}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=164.27us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:01,866] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1723 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:06,882] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5831.0 vnc_pixels_ps[total]=43244.9 reward_lag=None rewarder_message_lag=None fps=60.00\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:06,883] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"86.05us\", \"mean\": \"150.11us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"28.12us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"36.66us\", \"mean\": \"99.82us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"277.01us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"66.73us\", \"mean\": \"205.26us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"152.78us\", \"mean\": \"473.23us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"267.00us\", \"mean\": \"414.58us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"60.57us\", \"mean\": \"147.11us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=191.93us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:06,884] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1717 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:11,898] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5958.3 vnc_pixels_ps[total]=44209.4 reward_lag=None rewarder_message_lag=None fps=60.03\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:11,898] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"27.98us\", \"mean\": \"113.84us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"25.88us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"23.14us\", \"mean\": \"75.71us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"278.40us\", \"mean\": \"16.10ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"47.23us\", \"mean\": \"159.38us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"95.47us\", \"mean\": \"380.24us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"194.99us\", \"mean\": \"369.13us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"46.68us\", \"mean\": \"136.51us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=151.87us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:11,899] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1716 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:16,915] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5959.7 vnc_pixels_ps[total]=44182.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:16,916] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"26.52us\", \"mean\": \"129.54us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"33.49us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"21.49us\", \"mean\": \"83.55us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"273.01us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"44.36us\", \"mean\": \"171.91us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"111.93us\", \"mean\": \"452.00us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"262.79us\", \"mean\": \"398.23us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"80.65us\", \"mean\": \"144.25us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=243.19us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:16,917] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1719 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:21,931] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5931.4 vnc_pixels_ps[total]=44002.2 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:21,932] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"48.27us\", \"mean\": \"128.35us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"24.65us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"22.78us\", \"mean\": \"76.44us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"308.87us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"44.58us\", \"mean\": \"159.37us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"123.74us\", \"mean\": \"400.40us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"210.03us\", \"mean\": \"385.16us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"48.04us\", \"mean\": \"137.09us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=154.02us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:21,932] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1715 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:26,948] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5993.5 vnc_pixels_ps[total]=44480.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:26,950] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"38.90us\", \"mean\": \"123.54us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"23.96us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"19.25us\", \"mean\": \"73.17us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"230.84us\", \"mean\": \"16.10ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"42.28us\", \"mean\": \"155.59us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"92.38us\", \"mean\": \"398.94us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"230.61us\", \"mean\": \"378.44us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"47.96us\", \"mean\": \"134.44us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910307, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=228.64us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:26,950] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0.0 reward_min=0.0 reward_max=0.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:31,964] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6202.2 vnc_pixels_ps[total]=45977.7 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:31,965] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"31.04us\", \"mean\": \"127.37us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"34.25us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"25.03us\", \"mean\": \"77.49us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"300.66us\", \"mean\": \"16.13ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"56.29us\", \"mean\": \"165.77us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"101.25us\", \"mean\": \"411.49us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"210.28us\", \"mean\": \"349.18us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"39.31us\", \"mean\": \"122.44us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289038, \"std\": 0.2660985088079446}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=194.31us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:31,965] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:36,982] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5966.7 vnc_pixels_ps[total]=44274.2 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:36,983] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"28.79us\", \"mean\": \"125.72us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"30.92us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"23.53us\", \"mean\": \"84.70us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"235.78us\", \"mean\": \"16.09ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"47.66us\", \"mean\": \"174.56us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"98.71us\", \"mean\": \"417.82us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"225.30us\", \"mean\": \"381.80us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"45.24us\", \"mean\": \"136.64us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289041, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=190.02us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:36,983] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:41,998] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5925.8 vnc_pixels_ps[total]=43958.8 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:41,999] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"48.54us\", \"mean\": \"125.10us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"28.09us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"35.73us\", \"mean\": \"79.41us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"291.88us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"77.82us\", \"mean\": \"168.16us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"176.48us\", \"mean\": \"424.54us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"222.44us\", \"mean\": \"393.76us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.66us\", \"mean\": \"141.38us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=180.24us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:41,999] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1723 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:47,015] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6030.9 vnc_pixels_ps[total]=44768.0 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:47,016] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"27.68us\", \"mean\": \"113.64us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"31.10us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"23.28us\", \"mean\": \"73.70us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"252.69us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"44.81us\", \"mean\": \"153.22us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"95.92us\", \"mean\": \"382.53us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"206.70us\", \"mean\": \"398.56us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"65.14us\", \"mean\": \"150.62us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289041, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=254.39us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:47,016] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:52,031] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5899.0 vnc_pixels_ps[total]=43753.2 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:52,031] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"39.84us\", \"mean\": \"134.52us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"37.72us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"87.33us\", \"mean\": \"104.27us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"306.21us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"113.70us\", \"mean\": \"201.72us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"181.05us\", \"mean\": \"479.57us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"250.25us\", \"mean\": \"402.48us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"59.56us\", \"mean\": \"143.46us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289044, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=128.27us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:52,032] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1719 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:57,048] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5988.0 vnc_pixels_ps[total]=44438.4 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:57,048] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"28.78us\", \"mean\": \"115.88us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"31.64us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"47.11us\", \"mean\": \"81.49us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"231.57us\", \"mean\": \"16.09ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"87.17us\", \"mean\": \"169.15us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"118.49us\", \"mean\": \"394.79us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"207.92us\", \"mean\": \"383.42us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"58.59us\", \"mean\": \"142.44us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=144.00us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:26:57,048] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1719 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:02,064] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6243.5 vnc_pixels_ps[total]=46295.6 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:02,065] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"29.77us\", \"mean\": \"116.01us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"36.64us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"26.18us\", \"mean\": \"77.55us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"231.40us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"46.09us\", \"mean\": \"160.33us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"115.02us\", \"mean\": \"396.13us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"200.46us\", \"mean\": \"389.09us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.75us\", \"mean\": \"141.39us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910303, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=94.89us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:02,065] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1717 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:07,082] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5808.8 vnc_pixels_ps[total]=43058.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:07,082] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"23.82us\", \"mean\": \"103.94us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"24.69us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"18.47us\", \"mean\": \"68.77us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"199.12us\", \"mean\": \"16.10ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"40.80us\", \"mean\": \"145.04us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"91.79us\", \"mean\": \"353.65us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"193.18us\", \"mean\": \"373.87us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"48.47us\", \"mean\": \"135.63us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.26609850880794456}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=207.19us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:07,083] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1717 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:12,098] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5851.6 vnc_pixels_ps[total]=43458.5 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:12,099] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"50.56us\", \"mean\": \"132.31us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"31.76us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"24.25us\", \"mean\": \"80.02us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"272.51us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"48.59us\", \"mean\": \"166.62us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"183.34us\", \"mean\": \"441.47us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"232.42us\", \"mean\": \"401.02us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"54.24us\", \"mean\": \"144.18us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289038, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=209.33us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:12,100] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:17,115] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5964.0 vnc_pixels_ps[total]=44268.4 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:17,116] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"43.28us\", \"mean\": \"129.80us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"28.11us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"23.74us\", \"mean\": \"79.47us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"265.47us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"52.14us\", \"mean\": \"170.85us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"135.82us\", \"mean\": \"421.78us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"226.55us\", \"mean\": \"394.72us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"50.68us\", \"mean\": \"142.14us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289041, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=210.76us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:17,116] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:22,131] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6047.8 vnc_pixels_ps[total]=44823.4 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:22,131] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"34.22us\", \"mean\": \"122.65us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"32.26us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"45.39us\", \"mean\": \"86.30us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"281.93us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"68.86us\", \"mean\": \"175.28us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"100.23us\", \"mean\": \"399.02us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"216.85us\", \"mean\": \"403.47us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"65.08us\", \"mean\": \"146.97us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=119.69us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:22,131] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1717 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:27,149] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5803.2 vnc_pixels_ps[total]=43086.4 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:27,150] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"33.65us\", \"mean\": \"113.59us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"31.97us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"30.39us\", \"mean\": \"76.40us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"200.21us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"59.22us\", \"mean\": \"158.58us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"93.20us\", \"mean\": \"374.53us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"206.58us\", \"mean\": \"396.30us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.03us\", \"mean\": \"144.31us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=362.87us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:27,150] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:32,164] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5831.7 vnc_pixels_ps[total]=43229.9 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:32,165] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"23.45us\", \"mean\": \"116.28us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"27.89us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"20.54us\", \"mean\": \"78.03us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"281.52us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"41.53us\", \"mean\": \"162.50us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"83.34us\", \"mean\": \"386.26us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"184.47us\", \"mean\": \"394.82us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.27us\", \"mean\": \"143.91us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=123.74us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:32,165] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1723 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:37,181] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6245.4 vnc_pixels_ps[total]=46347.3 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:37,182] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"75.11us\", \"mean\": \"127.49us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"33.98us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"72.20us\", \"mean\": \"86.85us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"247.98us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"105.53us\", \"mean\": \"174.85us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"178.59us\", \"mean\": \"401.57us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"228.18us\", \"mean\": \"392.16us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"49.76us\", \"mean\": \"139.89us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910305, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=184.77us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:37,183] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:42,199] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5925.3 vnc_pixels_ps[total]=43955.5 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:42,200] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"32.04us\", \"mean\": \"107.61us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"26.35us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"28.78us\", \"mean\": \"69.16us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"240.08us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"59.17us\", \"mean\": \"145.76us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"111.75us\", \"mean\": \"363.11us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"214.39us\", \"mean\": \"384.48us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"50.13us\", \"mean\": \"137.63us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289038, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=325.20us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:42,201] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:47,215] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5876.8 vnc_pixels_ps[total]=43611.6 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:47,216] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"21.92us\", \"mean\": \"118.18us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"32.18us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"30.32us\", \"mean\": \"84.24us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"318.44us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"50.10us\", \"mean\": \"173.39us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"95.69us\", \"mean\": \"415.86us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"230.99us\", \"mean\": \"400.74us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"57.34us\", \"mean\": \"144.70us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=320.91us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:47,217] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:52,231] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6011.1 vnc_pixels_ps[total]=44540.5 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:52,232] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"23.37us\", \"mean\": \"108.17us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"26.90us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"34.89us\", \"mean\": \"77.15us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"284.44us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"49.20us\", \"mean\": \"154.48us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"75.76us\", \"mean\": \"366.70us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"196.07us\", \"mean\": \"383.74us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"58.13us\", \"mean\": \"139.50us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289042, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=232.93us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:52,233] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:57,247] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5909.5 vnc_pixels_ps[total]=43833.4 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:57,248] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"27.85us\", \"mean\": \"110.93us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"31.57us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"24.85us\", \"mean\": \"76.77us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"261.45us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"52.58us\", \"mean\": \"162.04us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"97.61us\", \"mean\": \"374.67us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"193.79us\", \"mean\": \"385.02us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"49.87us\", \"mean\": \"136.55us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=160.46us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:27:57,249] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:02,264] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5998.6 vnc_pixels_ps[total]=44519.8 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:02,265] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"27.34us\", \"mean\": \"110.57us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"27.29us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"27.54us\", \"mean\": \"77.24us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"237.23us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"58.28us\", \"mean\": \"164.96us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"120.57us\", \"mean\": \"382.91us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"204.20us\", \"mean\": \"384.71us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"57.17us\", \"mean\": \"139.12us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.2660985088079444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=174.76us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:02,265] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:07,282] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6159.0 vnc_pixels_ps[total]=45645.4 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:07,282] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"29.90us\", \"mean\": \"121.00us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"30.00us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"14.20us\", \"mean\": \"74.70us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"217.49us\", \"mean\": \"16.09ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"28.62us\", \"mean\": \"156.17us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"81.34us\", \"mean\": \"397.58us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"204.68us\", \"mean\": \"380.64us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"44.06us\", \"mean\": \"134.33us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910303, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=158.31us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:07,283] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *49 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *50 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *51 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *52 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *53 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *54 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *55 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *56 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *57 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:28:11 [info] 65#65: *58 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:12,298] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5973.5 vnc_pixels_ps[total]=44326.7 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:12,299] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"38.16us\", \"mean\": \"114.80us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"24.45us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"40.91us\", \"mean\": \"84.04us\"}, \"rewarder_protocol.latency.rtt.skew_unadjusted\": {\"calls\": 10, \"std\": \"370.62us\", \"mean\": \"835.32us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"252.52us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"64.71us\", \"mean\": \"170.23us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"105.13us\", \"mean\": \"380.10us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"214.11us\", \"mean\": \"390.15us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"55.94us\", \"mean\": \"140.40us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289041, \"std\": 0.26609850880794456}, \"rewarder_protocol.messages\": {\"calls\": 10, \"mean\": 1.0, \"std\": 0.0}, \"rewarder_protocol.messages.v0.control.ping\": {\"calls\": 10, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=277.28us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:12,299] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<2131 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:17,315] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5835.6 vnc_pixels_ps[total]=43264.9 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:17,315] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"38.02us\", \"mean\": \"122.79us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"31.89us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"51.18us\", \"mean\": \"85.75us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"275.15us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"68.09us\", \"mean\": \"171.83us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"118.10us\", \"mean\": \"411.72us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"225.29us\", \"mean\": \"401.55us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"60.97us\", \"mean\": \"142.00us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289037, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=216.96us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:17,316] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:22,331] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5957.1 vnc_pixels_ps[total]=44200.3 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:22,331] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"41.38us\", \"mean\": \"125.67us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"29.92us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"20.53us\", \"mean\": \"71.60us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"285.25us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"55.85us\", \"mean\": \"158.12us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"131.16us\", \"mean\": \"409.44us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"236.30us\", \"mean\": \"403.38us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"81.05us\", \"mean\": \"143.87us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=164.03us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:22,332] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1714 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:27,349] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5971.5 vnc_pixels_ps[total]=44311.3 reward_lag=None rewarder_message_lag=None fps=60.00\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:27,350] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"28.29us\", \"mean\": \"120.82us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"27.61us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"25.19us\", \"mean\": \"81.58us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"239.10us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"56.80us\", \"mean\": \"172.14us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"100.11us\", \"mean\": \"406.02us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"248.84us\", \"mean\": \"410.13us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"61.85us\", \"mean\": \"145.51us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=209.57us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:27,351] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1722 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:32,365] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5921.6 vnc_pixels_ps[total]=43926.5 reward_lag=None rewarder_message_lag=None fps=60.03\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:32,366] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"44.38us\", \"mean\": \"120.91us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"24.15us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"50.31us\", \"mean\": \"81.63us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"302.39us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"64.97us\", \"mean\": \"161.71us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"115.25us\", \"mean\": \"406.65us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"217.20us\", \"mean\": \"397.38us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"54.13us\", \"mean\": \"137.26us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=349.76us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:32,367] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1719 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:37,381] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5938.0 vnc_pixels_ps[total]=44068.1 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:37,382] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"25.35us\", \"mean\": \"108.97us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"25.58us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"19.25us\", \"mean\": \"73.71us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"283.66us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"45.16us\", \"mean\": \"160.03us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"81.99us\", \"mean\": \"369.99us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"204.93us\", \"mean\": \"404.62us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"69.50us\", \"mean\": \"145.19us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910307, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=123.50us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:37,382] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0.0 reward_min=0.0 reward_max=0.0 done=False info={'rewarder.vnc.updates.bytes': 1320, 'rewarder.profile': '<1722 bytes>', 'rewarder.vnc.updates.pixels': 9800, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:42,397] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6201.6 vnc_pixels_ps[total]=45973.2 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:42,398] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"47.37us\", \"mean\": \"138.82us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"24.53us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"18.63us\", \"mean\": \"78.38us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"253.60us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"72.90us\", \"mean\": \"179.40us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"114.07us\", \"mean\": \"444.30us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"215.69us\", \"mean\": \"394.90us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"52.99us\", \"mean\": \"139.30us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289038, \"std\": 0.26609850880794456}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=132.80us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:42,398] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1724 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:47,415] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5945.7 vnc_pixels_ps[total]=44112.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:47,416] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"27.15us\", \"mean\": \"125.41us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"25.71us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"23.63us\", \"mean\": \"82.47us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"249.63us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"49.81us\", \"mean\": \"175.81us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"95.83us\", \"mean\": \"419.87us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"247.41us\", \"mean\": \"404.59us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"75.31us\", \"mean\": \"143.42us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=398.16us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:47,417] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:52,432] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5934.2 vnc_pixels_ps[total]=43986.3 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:52,433] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"27.99us\", \"mean\": \"117.61us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"36.82us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"76.05us\", \"mean\": \"89.29us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"319.90us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"91.40us\", \"mean\": \"172.47us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"99.64us\", \"mean\": \"390.62us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"237.01us\", \"mean\": \"402.93us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"67.92us\", \"mean\": \"146.20us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=353.81us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:52,434] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:57,447] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5984.6 vnc_pixels_ps[total]=44411.8 reward_lag=None rewarder_message_lag=None fps=60.03\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:57,448] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"32.62us\", \"mean\": \"121.95us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"31.39us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"28.10us\", \"mean\": \"80.49us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"327.11us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"64.27us\", \"mean\": \"170.85us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"182.64us\", \"mean\": \"443.10us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"233.43us\", \"mean\": \"404.75us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"66.66us\", \"mean\": \"146.63us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289042, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=105.86us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:28:57,448] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:02,465] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5767.9 vnc_pixels_ps[total]=42759.0 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:02,465] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"30.30us\", \"mean\": \"117.07us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"27.51us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"19.06us\", \"mean\": \"71.69us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"247.37us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"41.29us\", \"mean\": \"152.42us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"136.00us\", \"mean\": \"398.49us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"224.12us\", \"mean\": \"403.78us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"70.93us\", \"mean\": \"145.68us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=231.98us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:02,466] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:07,483] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6015.0 vnc_pixels_ps[total]=44645.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:07,483] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"23.84us\", \"mean\": \"118.99us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"25.36us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"21.70us\", \"mean\": \"78.62us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"257.00us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"45.90us\", \"mean\": \"165.02us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"90.18us\", \"mean\": \"410.92us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"244.54us\", \"mean\": \"402.67us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"61.31us\", \"mean\": \"143.72us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.2660985088079444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=186.44us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:07,484] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1722 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:12,498] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6020.8 vnc_pixels_ps[total]=44758.4 reward_lag=None rewarder_message_lag=None fps=60.03\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:12,499] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"32.68us\", \"mean\": \"130.27us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"26.87us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"28.17us\", \"mean\": \"84.31us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"325.07us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"57.37us\", \"mean\": \"177.81us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"113.51us\", \"mean\": \"423.27us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"256.40us\", \"mean\": \"418.18us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"92.92us\", \"mean\": \"150.85us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910303, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=227.45us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:12,499] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:17,516] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5895.7 vnc_pixels_ps[total]=43742.2 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:17,517] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"37.07us\", \"mean\": \"127.96us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"125.07us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"26.98us\", \"mean\": \"80.83us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"307.02us\", \"mean\": \"16.04ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"56.93us\", \"mean\": \"171.37us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"142.06us\", \"mean\": \"421.13us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"278.15us\", \"mean\": \"422.72us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"57.61us\", \"mean\": \"147.54us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289041, \"std\": 0.26609850880794456}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=522.61us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:17,518] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:22,531] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5922.1 vnc_pixels_ps[total]=43930.1 reward_lag=None rewarder_message_lag=None fps=60.03\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:22,531] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"30.01us\", \"mean\": \"109.77us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"33.21us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"29.82us\", \"mean\": \"71.26us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"305.24us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"57.75us\", \"mean\": \"150.54us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"93.28us\", \"mean\": \"362.85us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"173.65us\", \"mean\": \"405.06us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"54.67us\", \"mean\": \"148.15us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289037, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=117.54us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:22,532] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:27,548] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5831.4 vnc_pixels_ps[total]=43247.5 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:27,549] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"38.96us\", \"mean\": \"124.41us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"26.49us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"25.55us\", \"mean\": \"72.41us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"215.96us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"50.50us\", \"mean\": \"151.21us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"116.70us\", \"mean\": \"391.58us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"200.26us\", \"mean\": \"411.94us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"54.22us\", \"mean\": \"149.20us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=390.53us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:27,550] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1714 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:32,564] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5991.9 vnc_pixels_ps[total]=44430.9 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:32,564] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"34.88us\", \"mean\": \"124.88us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"30.01us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"42.35us\", \"mean\": \"84.86us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"278.10us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"82.64us\", \"mean\": \"175.57us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"123.17us\", \"mean\": \"412.95us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"211.04us\", \"mean\": \"410.09us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"55.55us\", \"mean\": \"145.31us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=124.22us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:32,565] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:37,582] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5929.6 vnc_pixels_ps[total]=43989.1 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:37,582] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"115.42us\", \"mean\": \"145.77us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"23.63us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"48.47us\", \"mean\": \"91.23us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"247.59us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"73.24us\", \"mean\": \"184.30us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"167.16us\", \"mean\": \"446.36us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"249.06us\", \"mean\": \"410.75us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"52.02us\", \"mean\": \"143.27us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=119.92us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:37,583] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1716 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:42,597] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5989.6 vnc_pixels_ps[total]=44450.4 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:42,598] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"30.50us\", \"mean\": \"118.60us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"27.84us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"36.64us\", \"mean\": \"88.13us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"277.74us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"69.80us\", \"mean\": \"182.02us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"115.50us\", \"mean\": \"406.48us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"221.86us\", \"mean\": \"404.58us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"57.84us\", \"mean\": \"143.06us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910307, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=101.80us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:42,598] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0.0 reward_min=0.0 reward_max=0.0 done=False info={'rewarder.vnc.updates.bytes': 1346, 'rewarder.profile': '<1722 bytes>', 'rewarder.vnc.updates.pixels': 10000, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:47,614] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6208.5 vnc_pixels_ps[total]=46063.8 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:47,615] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"61.28us\", \"mean\": \"136.97us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"31.96us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"36.33us\", \"mean\": \"88.77us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"245.44us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"66.07us\", \"mean\": \"182.87us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"136.25us\", \"mean\": \"442.67us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"221.23us\", \"mean\": \"410.63us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"56.72us\", \"mean\": \"145.73us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289038, \"std\": 0.26609850880794456}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=249.62us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:47,616] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1727 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:52,631] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6020.2 vnc_pixels_ps[total]=44686.3 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:52,631] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"38.08us\", \"mean\": \"126.93us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"44.23us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"38.36us\", \"mean\": \"86.35us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"284.88us\", \"mean\": \"16.03ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"80.13us\", \"mean\": \"179.02us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"147.00us\", \"mean\": \"435.31us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"220.36us\", \"mean\": \"424.40us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"61.72us\", \"mean\": \"152.47us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=120.64us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:52,631] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1722 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:57,648] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5887.7 vnc_pixels_ps[total]=43666.3 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:57,649] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"46.65us\", \"mean\": \"132.15us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"25.23us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"42.74us\", \"mean\": \"86.20us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"224.56us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"67.93us\", \"mean\": \"172.47us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"120.33us\", \"mean\": \"423.02us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"221.19us\", \"mean\": \"409.68us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.54us\", \"mean\": \"143.23us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=193.60us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:29:57,649] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1713 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:02,664] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5965.4 vnc_pixels_ps[total]=44226.7 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:02,665] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"29.25us\", \"mean\": \"119.79us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"28.34us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"23.94us\", \"mean\": \"72.94us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"259.22us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"48.51us\", \"mean\": \"154.92us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"101.96us\", \"mean\": \"398.43us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"199.72us\", \"mean\": \"406.45us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.17us\", \"mean\": \"147.09us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289042, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=148.77us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:02,665] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1719 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:07,681] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5920.0 vnc_pixels_ps[total]=43914.8 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:07,681] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"27.42us\", \"mean\": \"112.33us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"26.34us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"18.50us\", \"mean\": \"70.42us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"228.96us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"57.16us\", \"mean\": \"154.92us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"106.76us\", \"mean\": \"385.37us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"191.63us\", \"mean\": \"405.94us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.00us\", \"mean\": \"145.76us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=101.80us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:07,681] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1719 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:12,698] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5824.3 vnc_pixels_ps[total]=43247.9 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:12,699] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"25.45us\", \"mean\": \"121.49us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"25.57us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"23.34us\", \"mean\": \"77.09us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"214.82us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"46.77us\", \"mean\": \"160.04us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"89.76us\", \"mean\": \"399.94us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"206.74us\", \"mean\": \"409.69us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.30us\", \"mean\": \"146.93us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.2660985088079444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=270.13us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:12,700] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1723 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:17,715] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6158.5 vnc_pixels_ps[total]=45693.4 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:17,716] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"23.59us\", \"mean\": \"112.89us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"23.90us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"24.05us\", \"mean\": \"75.37us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"271.64us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"45.24us\", \"mean\": \"157.03us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"92.84us\", \"mean\": \"383.94us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"225.48us\", \"mean\": \"402.44us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"57.12us\", \"mean\": \"145.40us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910303, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=312.33us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:17,716] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1722 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:22,731] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5889.3 vnc_pixels_ps[total]=43677.9 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:22,732] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"28.88us\", \"mean\": \"119.74us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"25.87us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"28.69us\", \"mean\": \"81.52us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"272.08us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"70.03us\", \"mean\": \"174.66us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"138.76us\", \"mean\": \"417.74us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"214.73us\", \"mean\": \"408.39us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"48.95us\", \"mean\": \"147.17us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289041, \"std\": 0.26609850880794456}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=203.85us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:22,732] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1717 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:27,747] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5988.8 vnc_pixels_ps[total]=44444.0 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:27,747] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"27.89us\", \"mean\": \"122.79us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"28.72us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"22.04us\", \"mean\": \"81.91us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"251.27us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"46.32us\", \"mean\": \"173.15us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"90.17us\", \"mean\": \"423.02us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"209.10us\", \"mean\": \"403.03us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"46.83us\", \"mean\": \"142.04us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289037, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=95.61us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:27,748] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1722 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:32,764] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5755.4 vnc_pixels_ps[total]=42718.0 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:32,765] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"24.12us\", \"mean\": \"113.48us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"21.55us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"26.12us\", \"mean\": \"77.70us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"206.19us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"50.88us\", \"mean\": \"161.72us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"98.06us\", \"mean\": \"378.76us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"194.17us\", \"mean\": \"393.18us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"52.19us\", \"mean\": \"145.27us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=129.22us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:32,765] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:37,782] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6038.4 vnc_pixels_ps[total]=44788.6 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:37,783] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"28.44us\", \"mean\": \"114.21us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"21.73us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"26.17us\", \"mean\": \"76.01us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"229.79us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"53.98us\", \"mean\": \"159.24us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"111.39us\", \"mean\": \"382.54us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"228.00us\", \"mean\": \"405.23us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"60.38us\", \"mean\": \"150.13us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=373.60us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:37,784] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1722 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:42,798] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5934.3 vnc_pixels_ps[total]=43987.0 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:42,798] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"33.27us\", \"mean\": \"116.97us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"24.16us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"43.04us\", \"mean\": \"83.49us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"308.58us\", \"mean\": \"16.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"61.97us\", \"mean\": \"166.17us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"131.83us\", \"mean\": \"396.47us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"212.93us\", \"mean\": \"404.37us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"62.95us\", \"mean\": \"150.87us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=168.32us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:42,799] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1725 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [nginx] 2019/03/18 08:30:44 [info] 65#65: *59 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT clients2.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:47,814] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5893.5 vnc_pixels_ps[total]=43781.1 reward_lag=None rewarder_message_lag=None fps=60.02\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:47,815] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"35.06us\", \"mean\": \"119.43us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"23.76us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"27.38us\", \"mean\": \"78.65us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"254.34us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"66.53us\", \"mean\": \"171.03us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"117.80us\", \"mean\": \"399.52us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"210.64us\", \"mean\": \"395.53us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.03us\", \"mean\": \"143.64us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910307, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=101.09us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:47,815] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0.0 reward_min=0.0 reward_max=0.0 done=False info={'rewarder.vnc.updates.bytes': 1161, 'rewarder.profile': '<1720 bytes>', 'rewarder.vnc.updates.pixels': 8576, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:52,831] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6087.4 vnc_pixels_ps[total]=45201.8 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:52,831] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"37.69us\", \"mean\": \"125.28us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"23.21us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"28.09us\", \"mean\": \"81.08us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"229.63us\", \"mean\": \"16.06ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"58.45us\", \"mean\": \"174.78us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"112.79us\", \"mean\": \"413.65us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"208.67us\", \"mean\": \"399.25us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"51.49us\", \"mean\": \"143.71us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289038, \"std\": 0.26609850880794456}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=184.77us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:52,832] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1725 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:57,848] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5819.0 vnc_pixels_ps[total]=43207.5 reward_lag=None rewarder_message_lag=None fps=60.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:57,850] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"22.11us\", \"mean\": \"114.11us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"26.57us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"40.60us\", \"mean\": \"84.79us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"219.72us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"57.66us\", \"mean\": \"169.31us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"82.63us\", \"mean\": \"393.09us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"222.83us\", \"mean\": \"396.99us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"50.30us\", \"mean\": \"144.54us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=350.00us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:30:57,850] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:02,864] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5963.2 vnc_pixels_ps[total]=44247.1 reward_lag=None rewarder_message_lag=None fps=60.03\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:02,864] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"21.18us\", \"mean\": \"113.18us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"24.86us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"17.42us\", \"mean\": \"73.36us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"284.79us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"35.48us\", \"mean\": \"154.85us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"67.61us\", \"mean\": \"383.47us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"184.26us\", \"mean\": \"386.87us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"53.68us\", \"mean\": \"144.49us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.0764119601328904, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=178.81us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:02,865] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1714 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:07,881] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5888.0 vnc_pixels_ps[total]=43668.2 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:07,881] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"43.98us\", \"mean\": \"125.69us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"26.98us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"26.06us\", \"mean\": \"85.85us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"242.98us\", \"mean\": \"16.04ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"58.83us\", \"mean\": \"187.77us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"96.19us\", \"mean\": \"421.54us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"220.28us\", \"mean\": \"411.77us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"57.05us\", \"mean\": \"151.27us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289042, \"std\": 0.26609850880794444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=128.75us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:07,882] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1723 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:12,897] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5926.6 vnc_pixels_ps[total]=43980.7 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:12,898] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"28.94us\", \"mean\": \"117.72us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"24.68us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"47.74us\", \"mean\": \"85.74us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"236.58us\", \"mean\": \"16.07ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"76.09us\", \"mean\": \"174.28us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"94.67us\", \"mean\": \"394.64us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"208.28us\", \"mean\": \"387.79us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"53.92us\", \"mean\": \"142.15us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289045, \"std\": 0.2660985088079445}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=151.16us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:12,898] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1718 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:17,916] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5829.6 vnc_pixels_ps[total]=43219.0 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:17,917] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"28.43us\", \"mean\": \"118.17us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"260.12us\", \"mean\": \"16.80ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"18.84us\", \"mean\": \"74.00us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"331.86us\", \"mean\": \"16.04ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"38.54us\", \"mean\": \"156.92us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"96.95us\", \"mean\": \"396.26us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"256.27us\", \"mean\": \"409.58us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"58.66us\", \"mean\": \"149.07us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289046, \"std\": 0.2660985088079444}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=251.53us)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:17,918] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1725 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:22,931] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=4.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=6259.2 vnc_pixels_ps[total]=46452.9 reward_lag=None rewarder_message_lag=None fps=60.03\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:22,931] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 24, \"std\": \"29.83us\", \"mean\": \"109.22us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"29.29us\", \"mean\": \"16.78ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 24, \"std\": \"22.89us\", \"mean\": \"70.85us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"340.80us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 24, \"std\": \"56.14us\", \"mean\": \"152.51us\"}, \"reward.parsing.score\": {\"calls\": 24, \"std\": \"114.16us\", \"mean\": \"372.63us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"202.51us\", \"mean\": \"372.54us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"46.88us\", \"mean\": \"134.88us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07973421926910303, \"std\": 0.27133238372607077}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 24, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 24, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=186.68us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:22,932] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1713 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:27,948] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=4.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=5893.3 vnc_pixels_ps[total]=43709.2 reward_lag=None rewarder_message_lag=None fps=60.01\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:27,949] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"score.crop_cache.get.OCRScorerV0\": {\"calls\": 23, \"std\": \"41.36us\", \"mean\": \"125.07us\"}, \"rewarder.frame\": {\"calls\": 301, \"std\": \"27.63us\", \"mean\": \"16.77ms\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 23, \"std\": \"18.78us\", \"mean\": \"76.54us\"}, \"rewarder.sleep\": {\"calls\": 301, \"std\": \"242.01us\", \"mean\": \"16.08ms\"}, \"reward.parsing.gameover\": {\"calls\": 23, \"std\": \"40.63us\", \"mean\": \"162.32us\"}, \"reward.parsing.score\": {\"calls\": 23, \"std\": \"89.76us\", \"mean\": \"404.86us\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"219.85us\", \"mean\": \"390.48us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"58.24us\", \"mean\": \"141.41us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"mean\": 0.0, \"std\": 0}, \"reward.vnc.updates.n\": {\"calls\": 301, \"mean\": 0.07641196013289041, \"std\": 0.26609850880794456}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 23, \"mean\": 1.0, \"std\": 0.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 23, \"mean\": 41.0, \"std\": 0.0, \"value\": 41.0}} (export_time=296.59us)\n",
      "\u001b[36muniverse-35kzJy-0 |\u001b[0m [2019-03-18 08:31:27,950] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.02s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.profile': '<1721 bytes>', 'rewarder.vnc.updates.pixels': 0, 'rewarder.vnc.updates.n': 0}\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    turn -= 1\n",
    "    \n",
    "    # Let us say initially we take no turn and move forward.\n",
    "    # First, We will check the value of turn, if it is less than 0\n",
    "    # then there is no necessity for turning and we just move forward\n",
    "    \n",
    "    if turn <= 0:\n",
    "        action = forward\n",
    "        turn = 0\n",
    "    \n",
    "    action_n = [action for ob in observation_n]\n",
    "    \n",
    "    # Then we use env.step() to perform an action (moving forward for now) one-time step\n",
    "    \n",
    "    observation_n, reward_n, done_n, info = env.step(action_n)\n",
    "    \n",
    "    # store the rewards in the rewards list\n",
    "    rewards += [reward_n[0]]\n",
    "     \n",
    "    # We will generate some random number and if it is less than 0.5 then we will take right, else\n",
    "    # we will take left and we will store all the rewards obtained by performing each action and\n",
    "    # based on our rewards we will learn which direction is the best over several timesteps. \n",
    "    \n",
    "    if len(rewards) >= buffer_size:\n",
    "        mean = sum(rewards)/len(rewards)\n",
    "        \n",
    "        if mean == 0:\n",
    "            turn = 20\n",
    "            if random.random() < 0.5:\n",
    "                 action = right\n",
    "            else:\n",
    "                action = left\n",
    "        rewards = []\n",
    "        \n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
