{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Agent to Play Atari games using Deep Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First we import all the necessary libraries </font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we define a function called preprocess_observation for preprocessing our input game screen. We reduce the image size\n",
    "and convert the image into greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.33333333333334"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.array([210, 164, 74]).mean()\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "\n",
    "    # Crop and resize the image\n",
    "    img = obs[1:176:2, ::2]\n",
    "\n",
    "    # Convert the image to greyscale\n",
    "    img = img.mean(axis=2)\n",
    "\n",
    "    # Improve image contrast\n",
    "    img[img==color] = 0\n",
    "\n",
    "    # Next we normalize the image from -1 to +1\n",
    "    img = (img - 128) / 128 - 1\n",
    "\n",
    "    return img.reshape(88,80,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us initialize our gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP',\n",
       " 'UP',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'DOWN',\n",
       " 'UPRIGHT',\n",
       " 'UPLEFT',\n",
       " 'DOWNRIGHT',\n",
       " 'DOWNLEFT']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.get_action_meanings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-25 12:20:12,648] Making new env: MsPacman-v0\n",
      "/home/b/anaconda3/envs/rl/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "n_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, _ = env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 80)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(preprocess_observation(obs)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fae713b8080>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fae71393240>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAAD8CAYAAACW2VP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEJlJREFUeJzt3W+MHdV5x/HvrzYWMSkyNoa4tmuDhAioEia7otCtKgqh0IgCLwLFpCgCI79JW9KkCpAXTSI1EpGqJLyoUC3bqSPxtwQUhCIockBJrcrFC2kJGGpCFnBw7IVCkyYiyOHpixmH9fre3bl35s495+7vI632zuzcO+fcuc+ec2fOPEcRgZml77eGXQAzq8bBapYJB6tZJhysZplwsJplwsFqlgkHq1kmagWrpMskvSjpJUm3NlUoMzuW+h0UIWkR8N/AJcB+4ClgY0Q831zxzOyIxTWeex7wUkS8DCDpXuBKoGuwnnzyybFu3boauzQbLa+88gpvvPGGqmxbJ1hXA6/NWN4P/P5cT1i3bh27du2qsUuz0TIxMVF52zrB2um/wTF9akmbgc0Aa9euPeYJN954Y40itG/79u3HrMutDrma/d7n+L53+vxUVecE035gZvStAV6fvVFEbImI8YgYX7lyZY3dmS1sdYL1KeAMSadJWgJcCzzcTLHMbLa+u8ERcVjSXwKPAYuA7RHxXGMlM7Oj1PnOSkR8B/hOQ2UxsznUCtZBqXoSZ1jbVZFjHXIscypla+PEo4cbmmXCwWqWib6HG/ZjbGwsZg+KyO1ama+zDs8oXmedmJhgcnKy0ggmt6xmmXCwmmXCwWqWCQerWSYcrGaZcLCaZSLJEUxV1bndKJXT/nXqkKNReN89gsnM5uRgNcuEg9UsEw5Ws0xkfYKpacO4Vcv8vlc1b8sqabukQ5J+OGPdckmPS9pX/j5psMU0syrd4H8GLpu17lZgZ0ScAewsl81sgObtBkfE9yStn7X6SuDC8vEO4EnglgbLNRRNdpdGpevVhlF439vYb78nmE6NiAMA5e9Tum0oabOkPZL2TE9P97k7Mxv42WDnDTZrRr/BelDSKoDy96HmimRmnfQbrA8DnywffxL4djPFMbNuqly6uQf4d+BMSfslbQJuBy6RtI9iysfbB1tMM6tyNnhjlz9d3HBZzGwOzm5o1iJnNzRbABysZplwsJplwsFqlokkb5FLeSawlMvWy3adpF7mKtNnpFK2btvV4ZbVLBMOVrNM+DqrWYt8ndVsAXCwmmXCwWqWCQerWSYcrGaZcLCaZWLeEUyS1gLfBD4EvAdsiYg7JC0H7gPWA1PANRHxVq8FqDqbV51LPINOIl1V03VIyTDqlspnotvrNa1Ky3oY+GxEnAWcD3xK0tk4d7BZq+YN1og4EBFPl49/DuwFVlPkDt5RbrYDuGpQhTSzHr+zlsm+zwV200PuYDOrr3KwSvog8C3g0xHxsx6e5yTfZg2odIucpOMoAvWuiHiwXH1Q0qqIODBX7uCI2AJsgWJscANl/o02ThIMeuzyKM96NgrvZ0rHp0oqUgHbgL0R8dUZf3LuYLMWVWlZJ4DrgWcl/aBc93mKXMH3l3mEXwWuHkwRu2v6P9ww/mOOSivaySi8nykdnyp5g/8N6HYLj3MHm7XEI5jMMuFgNcuEg9UsEw5Ws0w4WM0y4WA1y0SrSb6npqaSum7Vj2GVv6n9btq06ajlbdu2NfK6deT+mYD+6zA1NVV5W7esZplIcvoMa9bM1vRISzq7hbX0uWU1y4SD1SwTSXaDhzVz16jOIjd7ypJuUirzoGeRq8qzyJlZz1qdmGrFihVx6aWXtra/I1K6gXgYqp5MSuEyTltS+Uw89thjvPnmm56YymyUOFjNMlElrcvxkv5D0n9Kek7Sl8r1p0naLWmfpPskLRl8cc0Wriot66+AiyLiHGADcJmk84GvAF8rk3y/Bfgqu9kAVUnrEsD/lYvHlT8BXARcV67fAXwRuLP5IlpdC+nE0Sir9J1V0qIyWdoh4HHgR8DbEXG43GQ/RZZ+MxuQSsEaEb+OiA3AGuA84KxOm3V67swk3++8807/JTVb4HoawRQRb0t6kmKCqmWSFpet6xrg9S7POSrJ9zBmDOtkGLOypT5LWR2jXLcq+q3DxMRE5W2rnA1eKWlZ+fgDwEcpJqd6Avh4uZmTfJsNWJWWdRWwQ9IiiuC+PyIekfQ8cK+kvweeocjab2YDUuVs8H9RzBw3e/3LFN9fzawFHsFklokkb5GrKpXB2HXUqcOw6lr1ZMpCPz5Nc8tqlomsW9bc/kt3Mgp16GYU6pZSHdyymmXCwWqWCQerWSYcrGaZcLCaZcLBapYJB6tZJpKcRa7pW6ZSulZmxxrV41OlXp5FzmwEOVjNMuFgNcuEg9UsE5VPMJWZIvYAP4mIyyWdBtwLLAeeBq6PiHcHU8zOhjGb2bBmUMtRyrPI5Xh8emlZb6bIvXSEk3ybtajSLHKS1lAk8v4y8Bngz4Bp4EMRcVjSBcAXI2LOKeKqziKX0n+zVDT1ntx9991HLV933XVdthx8WUZJv+/JIGaR+zrwOeC9cnkFFZN8O2+wWTOqpCK9HDgUEZMzV3fYtGMTHRFbImI8IsaPP/74PotpZlVOME0AV0j6GHA8cCJFS1spybcN38yu75Fu7+zusKVv3pY1Im6LiDURsR64FvhuRHwCJ/k2a1Wd66y3AJ+R9BLFd1gn+TYboF7nunkSeLJ87CTfZi3yCCazTLR6i9z69euHMmNYKrOUpXAtcvaJpU4nn9o2qsenSr0anUXOzNLgYDXLhIPVLBNZT58xCmNU26hDSt9FfXz655bVLBMOVrNMZN0Nzq1L1UmdOqRyyaObhX58muaW1SwTDlazTDhYzTLhYDXLhIPVLBMOVrNMVM1uOAX8HPg1cDgixiUtB+4D1gNTwDUR8dZcr1M1u2HTUhqFMmz33HNP179t3LixxZIMVyqfiUFkNwT444jYEBHj5fKtwM4yb/DOctnMBqRON/hKilzClL+vql8cM+um6gimAP5VUgD/FBFbgFMj4gBARByQdMqgCmnNmd3VnatbbGmpGqwTEfF6GZCPS3qh6g4kbQY2AyxdurSPIpoZVAzWiHi9/H1I0kMUidIOSlpVtqqrgENdnrsF2ALFCaYq+xvWpEEpT0zVxnuSepmbnJiqqpQmsKqSkf8ESb995DHwJ8APgYcp8gWD8wabDVyVlvVU4CFJR7a/OyIelfQUcL+kTcCrwNWDK6aZVbrO2hRfZ01HpxNLvs46OtdZzWyIsr753OpbSK1p7tyymmXCwWqWCXeDFyh3f/PjltUsE9lMTJXKpZZhjZCp83p1pFy3VD4T0P/77ompzEaQg9UsEw5Ws0w4WM0ykfWlm5RP9qRctrpSrlvKZavLLatZJhysZpnIuhvcdHekyddLuWx1pVy3lMtWl1tWs0xUClZJyyQ9IOkFSXslXSBpuaTHJe0rf5806MKaLWRVW9Y7gEcj4sPAOcBenOTbrFVVEqadCPwRsA0gIt6NiLdxkm+zVlVpWU8HpoFvSHpG0tYyy+FRSb4BJ/k2G6AqwboY+AhwZ0ScC/yCHrq8kjZL2iNpz/T0dJ/FNLMql272A/sjYne5/ABFsPac5HtsbOyYVIptnBpP6fT7oC2kutYxrEs8dW5hnLdljYifAq9JOrNcdTHwPE7ybdaqqoMi/gq4S9IS4GXgBopAd5Jvs5ZUnevmB8B4hz9d3GxxzKwbj2Ayy0SSY4NTmaWs23ZV5DibWcqz9zX9Wil/drpxy2qWCQerWSZanUVubGwsdu3addS63K4LptwFTM1CSkVa1ew6TExMMDk56VnkzEaJg9UsEw5Ws0w4WM0y4WA1y4SD1SwTSY5gqqrO7UapnPZvuhwpzSKXynvcSY6fHbesZplwsJplwsFqlgkHq1km5j3BVKZzuW/GqtOBvwO+Wa5fD0wB10TEW00UaljjZUfhVq02pFy3UfjsdFMlB9OLEbEhIjYAY8AvgYdwkm+zVvV66eZi4EcR8YqkK4ELy/U7gCeBW5oo1LBajGFMkNT0dm1IuW6j8NnpptfvrNcC95SPneTbrEWVg7XMbHgF8C+97MBJvs2a0UvL+qfA0xFxsFw+WCb3Zr4k3xExHhHjK1eurFdaswWsl2DdyPtdYHCSb7NWVZ2fdSlwCfDgjNW3A5dI2lf+7fbmi2dmR1RN8v1LYMWsdW/iJN9mrfEIJrNMZH2LXCpSui0tJU2/LylLYgSTmaXBwWqWCQerWSYcrGaZ8AmmGVK6DW22lMqWUllmS7lsdbllNcuEW9YZUv4PnFLZUirLbL5FzsyGzsFqlgkHq1kmHKxmmXCwmmXCwWqWCQerWSYUEfNvJP0NcBMQwLPADcAq4F5gOfA0cH1EvDvX64yNjcWuXbuOWpfyNTuzps0eYTUxMcHk5KSqPHfellXSauCvgfGI+D1gEUVK0q8AXyuTfL8FbOqx3GbWg6rd4MXAByQtBpYCB4CLgAfKv+8Armq+eGZ2RJXpM34C/APwKkWQ/i8wCbwdEYfLzfYDqzs933mDzZpRpRt8EnAlcBrwO8AJFDmEZ+v45dd5g82aUWUg/0eBH0fENICkB4E/AJZJWly2rmuA15sq1LBmKauyXcpl62W7TlIv8+ztUi5bt+3qqPKd9VXgfElLJYki/ejzwBPAx8ttnOTbbMCqXrr5EvDnwGHgGYrLOKt5/9LNM8BfRMSv5nodX7qxha7OpZuqSb6/AHxh1uqXgfOqPN/M6vMIJrNMOFjNMuFgNcuEg9UsEw5Ws0w4WM0yMfRUpAtppjGzOtyymmWi0gimxnYmNbqzQ4cOAXDKKack+XqWhq1bt/7m8U033TTEknQWEc3cfG5maXCwmmUiu27wka5qN712YZt+PbNeuRtsNmLablmngV8Ab7S208E4mfzrAKNRj9zrsC4iKqVQaTVYASTtiYjxVnfasFGoA4xGPUahDlW5G2yWCQerWSaGEaxbhrDPpo1CHWA06jEKdaik9e+sZtYfd4PNMtFqsEq6TNKLkl6SdGub++6XpLWSnpC0V9Jzkm4u1y+X9LikfeXvk4Zd1vlIWiTpGUmPlMunSdpd1uE+SUuGXcb5SFom6QFJL5TH5IIcj0U/WgtWSYuAf6TI5n82sFHS2W3tv4bDwGcj4izgfOBTZblvBXaWE3PtLJdTdzOwd8ZyjpOL3QE8GhEfBs6hqE+Ox6J3EdHKD3AB8NiM5duA29raf4P1+DZwCfAisKpctwp4cdhlm6fcayg+yBcBjwCiGEywuNPxSfEHOBH4MeW5lhnrszoW/f602Q1eDbw2Y7nrZFapkrQeOBfYDZwaEQcAyt+pDyL+OvA54L1yeQUVJxdLyOnANPCNsju/VdIJ5Hcs+tJmsHYarJzNqWhJHwS+BXw6In427PL0QtLlwKGImJy5usOmqR+PxcBHgDsj4lyKoauj2eXtoM1g3Q+snbHc6GRWgyTpOIpAvSsiHixXH5S0qvz7KmDu23eGawK4QtIUxZQnF1G0tMvKOXchj+OxH9gfEbvL5QcogjenY9G3NoP1KeCM8gzkEorZ0x9ucf99KSfj2gbsjYivzvjTwxQTckHiE3NFxG0RsSYi1lO879+NiE+Q2eRiEfFT4DVJZ5arjkySls2xqKPtu24+RvEffRGwPSK+3NrO+yTpD4HvA8/y/ve9z1N8b70f+F2Kmfaujoj/GUoheyDpQuBvI+JySafT4+RiwyZpA7AVWEIx39INFI1OdseiVx7BZJYJj2Ayy4SD1SwTDlazTDhYzTLhYDXLhIPVLBMOVrNMOFjNMvH/F16PSJGyII0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.squeeze(preprocess_observation(obs)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Okay, Now we define a function called q_network for building our Q network. We input the game state\n",
    "to the Q network and get the Q values for all the actions in that state. <br><br>\n",
    "We build Q network with three convolutional layers with same padding followed by a fully connected layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def q_network(X, name_scope):\n",
    "    \n",
    "    # Initialize layers\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    with tf.variable_scope(name_scope) as scope: \n",
    "\n",
    "        # initialize the convolutional layers\n",
    "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer) \n",
    "        tf.summary.histogram('layer_1',layer_1)\n",
    "        \n",
    "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_2',layer_2)\n",
    "        \n",
    "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_3',layer_3)\n",
    "        \n",
    "        # Flatten the result of layer_3 before feeding to the fully connected layer\n",
    "        flat = flatten(layer_3)\n",
    "\n",
    "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
    "        tf.summary.histogram('fc',fc)\n",
    "        \n",
    "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "        tf.summary.histogram('output',output)\n",
    "        \n",
    "\n",
    "        # Vars will store the parameters of the network such as weights\n",
    "        vars = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} \n",
    "        return vars, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function called epsilon_greedy for performing epsilon greedy policy. In epsilon greedy policy we either select the best action with probability 1 - epsilon or a random action with\n",
    "probability epsilon.\n",
    "\n",
    "We use decaying epsilon greedy policy where value of epsilon will be decaying over time as we don't want to explore\n",
    "forever. So over time our policy will be exploiting only good actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5\n",
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 500000\n",
    "\n",
    "def epsilon_greedy(action, step):\n",
    "    p = np.random.random(1).squeeze()\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initialize our experience replay buffer of length 20000 which holds the experience.\n",
    "\n",
    "We store all the agent's experience i.e (state, action, rewards) in the experience replay buffer\n",
    "and  we sample from this minibatch of experience for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = deque(maxlen=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.rotate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([2, 0, 1])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_len = 20000\n",
    "exp_buffer = deque(maxlen=buffer_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function called sample_memories for sampling experiences from the memory. Batch size is the number of experience sampled\n",
    "from the memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, 1])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):\n",
    "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
    "    mem = np.array(exp_buffer)[perm_batch]\n",
    "    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our network hyperparameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 800\n",
    "batch_size = 48\n",
    "input_shape = (None, 88, 80, 1)\n",
    "learning_rate = 0.001\n",
    "X_shape = (None, 88, 80, 1)\n",
    "discount_factor = 0.97\n",
    "\n",
    "global_step = 0\n",
    "copy_steps = 100\n",
    "steps_train = 4\n",
    "start_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'logs2'\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Now we define the placeholder for our input i.e game state\n",
    "X = tf.placeholder(tf.float32, shape=X_shape)\n",
    "\n",
    "# we define a boolean called in_training_model to toggle the training\n",
    "in_training_mode = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let us build our primary and target Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we build our Q network, which takes the input X and generates Q values for all the actions in the state\n",
    "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
    "\n",
    "# similarly we build our target Q network\n",
    "targetQ, targetQ_outputs = q_network(X, 'targetQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the placeholder for our action values\n",
    "X_action = tf.placeholder(tf.int32, shape=(None,))\n",
    "Q_action = tf.reduce_sum(targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the primary Q network parameters to the target  Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_op = [tf.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
    "copy_target_to_main = tf.group(*copy_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and optimize loss using gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a placeholder for our output i.e action\n",
    "y = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "# now we calculate the loss which is the difference between actual value and predicted value\n",
    "loss = tf.reduce_mean(tf.square(y - Q_action))\n",
    "\n",
    "# we use adam optimizer for minimizing the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "loss_summary = tf.summary.scalar('LOSS', loss)\n",
    "merge_summary = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we start the tensorflow session and run the model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 736 Reward 220.0\n",
      "Epoch 806 Reward 290.0\n",
      "Epoch 519 Reward 250.0\n",
      "Epoch 643 Reward 250.0\n",
      "Epoch 551 Reward 220.0\n",
      "Epoch 599 Reward 230.0\n",
      "Epoch 661 Reward 240.0\n",
      "Epoch 608 Reward 190.0\n",
      "Epoch 719 Reward 250.0\n",
      "Epoch 615 Reward 190.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # for each episode\n",
    "    for i in range(num_episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        epoch = 0\n",
    "        episodic_reward = 0\n",
    "        actions_counter = Counter() \n",
    "        episodic_loss = []\n",
    "\n",
    "        # while the state is not the terminal state\n",
    "        while not done:\n",
    "\n",
    "           #env.render()\n",
    "        \n",
    "            # get the preprocessed game screen\n",
    "            obs = preprocess_observation(obs)\n",
    "\n",
    "            # feed the game screen and get the Q values for each action\n",
    "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
    "\n",
    "            # get the action\n",
    "            action = np.argmax(actions, axis=-1)\n",
    "            actions_counter[str(action)] += 1 \n",
    "\n",
    "            # select the action using epsilon greedy policy\n",
    "            action = epsilon_greedy(action, global_step)\n",
    "            \n",
    "            # now perform the action and move to the next state, next_obs, receive reward\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Store this transistion as an experience in the replay buffer\n",
    "            exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])\n",
    "            \n",
    "            # After certain steps, we train our Q network with samples from the experience replay buffer\n",
    "            if global_step % steps_train == 0 and global_step > start_steps:\n",
    "                \n",
    "                # sample experience\n",
    "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
    "\n",
    "                # states\n",
    "                o_obs = [x for x in o_obs]\n",
    "\n",
    "                # next states\n",
    "                o_next_obs = [x for x in o_next_obs]\n",
    "\n",
    "                # next actions\n",
    "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
    "\n",
    "\n",
    "                # reward\n",
    "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done) \n",
    "\n",
    "                # merge all summaries and write to the file\n",
    "                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
    "                file_writer.add_summary(mrg_summary, global_step)\n",
    "\n",
    "                # now we train the network and calculate loss\n",
    "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n",
    "                episodic_loss.append(train_loss)\n",
    "            \n",
    "            # after some interval we copy our main Q network weights to target Q network\n",
    "            if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
    "                copy_target_to_main.run()\n",
    "                \n",
    "            obs = next_obs\n",
    "            epoch += 1\n",
    "            global_step += 1\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        print('Epoch', epoch, 'Reward', episodic_reward,)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-122-10eb25ca27f4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-122-10eb25ca27f4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Epoch 333 Reward 70.0\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Epoch 333 Reward 70.0\n",
    "Epoch 882 Reward 240.0\n",
    "Epoch 1145 Reward 400.0\n",
    "Epoch 527 Reward 150.0\n",
    "Epoch 454 Reward 160.0\n",
    "Epoch 592 Reward 220.0\n",
    "Epoch 778 Reward 220.0\n",
    "Epoch 641 Reward 180.0\n",
    "Epoch 717 Reward 260.0\n",
    "Epoch 711 Reward 310.0\n",
    "Epoch 667 Reward 210.0\n",
    "Epoch 761 Reward 250.0\n",
    "Epoch 651 Reward 180.0\n",
    "Epoch 588 Reward 160.0\n",
    "Epoch 728 Reward 310.0\n",
    "Epoch 669 Reward 270.0\n",
    "Epoch 746 Reward 220.0\n",
    "Epoch 640 Reward 280.0\n",
    "Epoch 674 Reward 180.0\n",
    "Epoch 665 Reward 230.0\n",
    "Epoch 606 Reward 160.0\n",
    "Epoch 546 Reward 210.0\n",
    "Epoch 631 Reward 200.0\n",
    "Epoch 658 Reward 200.0\n",
    "Epoch 538 Reward 200.0\n",
    "Epoch 927 Reward 250.0\n",
    "Epoch 639 Reward 200.0\n",
    "Epoch 620 Reward 260.0\n",
    "Epoch 733 Reward 330.0\n",
    "Epoch 542 Reward 180.0\n",
    "Epoch 518 Reward 160.0\n",
    "Epoch 629 Reward 210.0\n",
    "Epoch 561 Reward 120.0\n",
    "Epoch 624 Reward 280.0\n",
    "Epoch 703 Reward 210.0\n",
    "Epoch 714 Reward 250.0\n",
    "Epoch 718 Reward 300.0\n",
    "Epoch 606 Reward 110.0\n",
    "Epoch 680 Reward 210.0\n",
    "Epoch 678 Reward 220.0\n",
    "Epoch 658 Reward 260.0\n",
    "Epoch 613 Reward 230.0\n",
    "Epoch 683 Reward 250.0\n",
    "Epoch 640 Reward 240.0\n",
    "Epoch 599 Reward 220.0\n",
    "Epoch 548 Reward 60.0\n",
    "Epoch 653 Reward 250.0\n",
    "Epoch 704 Reward 240.0\n",
    "Epoch 615 Reward 190.0\n",
    "Epoch 595 Reward 130.0\n",
    "Epoch 664 Reward 170.0\n",
    "Epoch 472 Reward 170.0\n",
    "Epoch 531 Reward 240.0\n",
    "Epoch 648 Reward 100.0\n",
    "Epoch 907 Reward 1010.0\n",
    "Epoch 440 Reward 130.0\n",
    "Epoch 650 Reward 260.0\n",
    "Epoch 657 Reward 180.0\n",
    "Epoch 633 Reward 220.0\n",
    "Epoch 767 Reward 300.0\n",
    "Epoch 541 Reward 180.0\n",
    "Epoch 709 Reward 260.0\n",
    "Epoch 1008 Reward 820.0\n",
    "Epoch 568 Reward 130.0\n",
    "Epoch 1133 Reward 370.0\n",
    "Epoch 560 Reward 190.0\n",
    "Epoch 640 Reward 190.0\n",
    "Epoch 666 Reward 130.0\n",
    "Epoch 746 Reward 260.0\n",
    "Epoch 784 Reward 370.0\n",
    "Epoch 689 Reward 300.0\n",
    "Epoch 517 Reward 160.0\n",
    "Epoch 817 Reward 430.0\n",
    "Epoch 643 Reward 200.0\n",
    "Epoch 544 Reward 110.0\n",
    "Epoch 675 Reward 270.0\n",
    "Epoch 671 Reward 340.0\n",
    "Epoch 639 Reward 240.0\n",
    "Epoch 650 Reward 300.0\n",
    "Epoch 716 Reward 340.0\n",
    "Epoch 662 Reward 290.0\n",
    "Epoch 931 Reward 550.0\n",
    "Epoch 634 Reward 220.0\n",
    "Epoch 813 Reward 320.0\n",
    "Epoch 640 Reward 270.0\n",
    "Epoch 513 Reward 210.0\n",
    "Epoch 698 Reward 290.0\n",
    "Epoch 645 Reward 330.0\n",
    "Epoch 727 Reward 280.0\n",
    "Epoch 520 Reward 90.0\n",
    "Epoch 664 Reward 260.0\n",
    "Epoch 625 Reward 170.0\n",
    "Epoch 663 Reward 200.0\n",
    "Epoch 523 Reward 190.0\n",
    "Epoch 663 Reward 280.0\n",
    "Epoch 649 Reward 220.0\n",
    "Epoch 973 Reward 650.0\n",
    "Epoch 456 Reward 150.0\n",
    "Epoch 939 Reward 380.0\n",
    "Epoch 617 Reward 140.0\n",
    "Epoch 812 Reward 290.0\n",
    "Epoch 734 Reward 480.0\n",
    "Epoch 677 Reward 300.0\n",
    "Epoch 728 Reward 330.0\n",
    "Epoch 522 Reward 160.0\n",
    "Epoch 669 Reward 180.0\n",
    "Epoch 634 Reward 190.0\n",
    "Epoch 669 Reward 250.0\n",
    "Epoch 629 Reward 200.0\n",
    "Epoch 559 Reward 160.0\n",
    "Epoch 491 Reward 150.0\n",
    "Epoch 563 Reward 190.0\n",
    "Epoch 659 Reward 210.0\n",
    "Epoch 695 Reward 190.0\n",
    "Epoch 585 Reward 240.0\n",
    "Epoch 707 Reward 290.0\n",
    "Epoch 647 Reward 240.0\n",
    "Epoch 687 Reward 290.0\n",
    "Epoch 617 Reward 170.0\n",
    "Epoch 672 Reward 220.0\n",
    "Epoch 666 Reward 200.0\n",
    "Epoch 579 Reward 220.0\n",
    "Epoch 703 Reward 310.0\n",
    "Epoch 504 Reward 160.0\n",
    "Epoch 1056 Reward 990.0\n",
    "Epoch 551 Reward 190.0\n",
    "Epoch 436 Reward 110.0\n",
    "Epoch 697 Reward 260.0\n",
    "Epoch 534 Reward 170.0\n",
    "Epoch 508 Reward 190.0\n",
    "Epoch 520 Reward 180.0\n",
    "Epoch 578 Reward 200.0\n",
    "Epoch 661 Reward 180.0\n",
    "Epoch 653 Reward 260.0\n",
    "Epoch 432 Reward 160.0\n",
    "Epoch 1069 Reward 1100.0\n",
    "Epoch 641 Reward 160.0\n",
    "Epoch 597 Reward 70.0\n",
    "Epoch 738 Reward 390.0\n",
    "Epoch 619 Reward 250.0\n",
    "Epoch 849 Reward 450.0\n",
    "Epoch 647 Reward 310.0\n",
    "Epoch 678 Reward 300.0\n",
    "Epoch 750 Reward 270.0\n",
    "Epoch 568 Reward 210.0\n",
    "Epoch 592 Reward 260.0\n",
    "Epoch 656 Reward 190.0\n",
    "Epoch 617 Reward 270.0\n",
    "Epoch 737 Reward 250.0\n",
    "Epoch 694 Reward 180.0\n",
    "Epoch 554 Reward 140.0\n",
    "Epoch 548 Reward 210.0\n",
    "Epoch 696 Reward 250.0\n",
    "Epoch 691 Reward 220.0\n",
    "Epoch 631 Reward 250.0\n",
    "Epoch 580 Reward 230.0\n",
    "Epoch 588 Reward 150.0\n",
    "Epoch 704 Reward 250.0\n",
    "Epoch 589 Reward 190.0\n",
    "Epoch 565 Reward 180.0\n",
    "Epoch 511 Reward 160.0\n",
    "Epoch 689 Reward 220.0\n",
    "Epoch 596 Reward 190.0\n",
    "Epoch 606 Reward 180.0\n",
    "Epoch 574 Reward 190.0\n",
    "Epoch 700 Reward 450.0\n",
    "Epoch 660 Reward 240.0\n",
    "Epoch 569 Reward 160.0\n",
    "Epoch 671 Reward 260.0\n",
    "Epoch 906 Reward 500.0\n",
    "Epoch 586 Reward 200.0\n",
    "Epoch 693 Reward 250.0\n",
    "Epoch 666 Reward 300.0\n",
    "Epoch 763 Reward 390.0\n",
    "Epoch 710 Reward 230.0\n",
    "Epoch 621 Reward 260.0\n",
    "Epoch 556 Reward 260.0\n",
    "Epoch 674 Reward 160.0\n",
    "Epoch 505 Reward 110.0\n",
    "Epoch 553 Reward 150.0\n",
    "Epoch 635 Reward 240.0\n",
    "Epoch 617 Reward 260.0\n",
    "Epoch 842 Reward 310.0\n",
    "Epoch 674 Reward 250.0\n",
    "Epoch 685 Reward 270.0\n",
    "Epoch 717 Reward 320.0\n",
    "Epoch 670 Reward 300.0\n",
    "Epoch 489 Reward 200.0\n",
    "Epoch 711 Reward 310.0\n",
    "Epoch 584 Reward 200.0\n",
    "Epoch 607 Reward 230.0\n",
    "Epoch 704 Reward 250.0\n",
    "Epoch 754 Reward 270.0\n",
    "Epoch 641 Reward 240.0\n",
    "Epoch 808 Reward 360.0\n",
    "Epoch 619 Reward 210.0\n",
    "Epoch 649 Reward 200.0\n",
    "Epoch 1030 Reward 780.0\n",
    "Epoch 820 Reward 320.0\n",
    "Epoch 615 Reward 250.0\n",
    "Epoch 585 Reward 210.0\n",
    "Epoch 1224 Reward 1380.0\n",
    "Epoch 703 Reward 250.0\n",
    "Epoch 823 Reward 760.0\n",
    "Epoch 713 Reward 350.0\n",
    "Epoch 912 Reward 470.0\n",
    "Epoch 639 Reward 250.0\n",
    "Epoch 717 Reward 240.0\n",
    "Epoch 673 Reward 290.0\n",
    "Epoch 435 Reward 150.0\n",
    "Epoch 572 Reward 250.0\n",
    "Epoch 645 Reward 290.0\n",
    "Epoch 675 Reward 230.0\n",
    "Epoch 540 Reward 190.0\n",
    "Epoch 554 Reward 240.0\n",
    "Epoch 782 Reward 410.0\n",
    "Epoch 477 Reward 170.0\n",
    "Epoch 576 Reward 220.0\n",
    "Epoch 1213 Reward 440.0\n",
    "Epoch 651 Reward 230.0\n",
    "Epoch 737 Reward 350.0\n",
    "Epoch 565 Reward 270.0\n",
    "Epoch 706 Reward 320.0\n",
    "Epoch 784 Reward 350.0\n",
    "Epoch 1052 Reward 460.0\n",
    "Epoch 528 Reward 240.0\n",
    "Epoch 732 Reward 290.0\n",
    "Epoch 459 Reward 130.0\n",
    "Epoch 646 Reward 250.0\n",
    "Epoch 791 Reward 490.0\n",
    "Epoch 801 Reward 450.0\n",
    "Epoch 517 Reward 180.0\n",
    "Epoch 954 Reward 800.0\n",
    "Epoch 700 Reward 240.0\n",
    "Epoch 744 Reward 200.0\n",
    "Epoch 513 Reward 180.0\n",
    "Epoch 656 Reward 270.0\n",
    "Epoch 651 Reward 410.0\n",
    "Epoch 696 Reward 320.0\n",
    "Epoch 767 Reward 250.0\n",
    "Epoch 878 Reward 350.0\n",
    "Epoch 450 Reward 250.0\n",
    "Epoch 787 Reward 300.0\n",
    "Epoch 576 Reward 210.0\n",
    "Epoch 1520 Reward 720.0\n",
    "Epoch 623 Reward 170.0\n",
    "Epoch 562 Reward 190.0\n",
    "Epoch 626 Reward 330.0\n",
    "Epoch 574 Reward 270.0\n",
    "Epoch 685 Reward 300.0\n",
    "Epoch 561 Reward 150.0\n",
    "Epoch 527 Reward 250.0\n",
    "Epoch 684 Reward 220.0\n",
    "Epoch 960 Reward 1730.0\n",
    "Epoch 598 Reward 350.0\n",
    "Epoch 735 Reward 250.0\n",
    "Epoch 513 Reward 180.0\n",
    "Epoch 656 Reward 430.0\n",
    "Epoch 752 Reward 300.0\n",
    "Epoch 534 Reward 240.0\n",
    "Epoch 568 Reward 240.0\n",
    "Epoch 800 Reward 390.0\n",
    "Epoch 628 Reward 170.0\n",
    "Epoch 710 Reward 350.0\n",
    "Epoch 505 Reward 170.0\n",
    "Epoch 689 Reward 270.0\n",
    "Epoch 580 Reward 270.0\n",
    "Epoch 597 Reward 160.0\n",
    "Epoch 635 Reward 270.0\n",
    "Epoch 920 Reward 550.0\n",
    "Epoch 770 Reward 350.0\n",
    "Epoch 880 Reward 1050.0\n",
    "Epoch 648 Reward 210.0\n",
    "Epoch 859 Reward 720.0\n",
    "Epoch 705 Reward 220.0\n",
    "Epoch 1095 Reward 490.0\n",
    "Epoch 679 Reward 370.0\n",
    "Epoch 649 Reward 330.0\n",
    "Epoch 616 Reward 540.0\n",
    "Epoch 517 Reward 170.0\n",
    "Epoch 934 Reward 500.0\n",
    "Epoch 845 Reward 450.0\n",
    "Epoch 1651 Reward 1020.0\n",
    "Epoch 825 Reward 410.0\n",
    "Epoch 684 Reward 270.0\n",
    "Epoch 937 Reward 560.0\n",
    "Epoch 547 Reward 240.0\n",
    "Epoch 718 Reward 360.0\n",
    "Epoch 981 Reward 400.0\n",
    "Epoch 835 Reward 560.0\n",
    "Epoch 779 Reward 550.0\n",
    "Epoch 776 Reward 700.0\n",
    "Epoch 815 Reward 600.0\n",
    "Epoch 613 Reward 230.0\n",
    "Epoch 787 Reward 310.0\n",
    "Epoch 591 Reward 140.0\n",
    "Epoch 503 Reward 160.0\n",
    "Epoch 699 Reward 380.0\n",
    "Epoch 1121 Reward 600.0\n",
    "Epoch 635 Reward 330.0\n",
    "Epoch 524 Reward 210.0\n",
    "Epoch 686 Reward 310.0\n",
    "Epoch 747 Reward 600.0\n",
    "Epoch 1212 Reward 860.0\n",
    "Epoch 595 Reward 190.0\n",
    "Epoch 371 Reward 90.0\n",
    "Epoch 540 Reward 190.0\n",
    "Epoch 640 Reward 320.0\n",
    "Epoch 733 Reward 420.0\n",
    "Epoch 547 Reward 240.0\n",
    "Epoch 659 Reward 380.0\n",
    "Epoch 740 Reward 380.0\n",
    "Epoch 824 Reward 420.0\n",
    "Epoch 657 Reward 220.0\n",
    "Epoch 732 Reward 310.0\n",
    "Epoch 626 Reward 340.0\n",
    "Epoch 577 Reward 260.0\n",
    "Epoch 694 Reward 360.0\n",
    "Epoch 771 Reward 420.0\n",
    "Epoch 906 Reward 590.0\n",
    "Epoch 475 Reward 230.0\n",
    "Epoch 1158 Reward 1500.0\n",
    "Epoch 885 Reward 600.0\n",
    "Epoch 585 Reward 330.0\n",
    "Epoch 532 Reward 220.0\n",
    "Epoch 775 Reward 570.0\n",
    "Epoch 492 Reward 190.0\n",
    "Epoch 519 Reward 260.0\n",
    "Epoch 855 Reward 580.0\n",
    "Epoch 758 Reward 450.0\n",
    "Epoch 760 Reward 390.0\n",
    "Epoch 613 Reward 250.0\n",
    "Epoch 687 Reward 340.0\n",
    "Epoch 529 Reward 180.0\n",
    "Epoch 582 Reward 270.0\n",
    "Epoch 852 Reward 420.0\n",
    "Epoch 1436 Reward 1140.0\n",
    "Epoch 1390 Reward 1220.0\n",
    "Epoch 637 Reward 390.0\n",
    "Epoch 461 Reward 170.0\n",
    "Epoch 590 Reward 210.0\n",
    "Epoch 782 Reward 450.0\n",
    "Epoch 491 Reward 170.0\n",
    "Epoch 637 Reward 280.0\n",
    "Epoch 698 Reward 370.0\n",
    "Epoch 728 Reward 600.0\n",
    "Epoch 662 Reward 350.0\n",
    "Epoch 744 Reward 530.0\n",
    "Epoch 798 Reward 750.0\n",
    "Epoch 624 Reward 300.0\n",
    "Epoch 660 Reward 340.0\n",
    "Epoch 635 Reward 370.0\n",
    "Epoch 576 Reward 280.0\n",
    "Epoch 640 Reward 370.0\n",
    "Epoch 529 Reward 250.0\n",
    "Epoch 742 Reward 260.0\n",
    "Epoch 728 Reward 470.0\n",
    "Epoch 656 Reward 320.0\n",
    "Epoch 682 Reward 550.0\n",
    "Epoch 610 Reward 380.0\n",
    "Epoch 1097 Reward 810.0\n",
    "Epoch 813 Reward 480.0\n",
    "Epoch 742 Reward 410.0\n",
    "Epoch 727 Reward 390.0\n",
    "Epoch 653 Reward 420.0\n",
    "Epoch 594 Reward 290.0\n",
    "Epoch 620 Reward 340.0\n",
    "Epoch 630 Reward 270.0\n",
    "Epoch 866 Reward 500.0\n",
    "Epoch 582 Reward 240.0\n",
    "Epoch 863 Reward 390.0\n",
    "Epoch 899 Reward 690.0\n",
    "Epoch 795 Reward 680.0\n",
    "Epoch 503 Reward 220.0\n",
    "Epoch 1013 Reward 1020.0\n",
    "Epoch 596 Reward 350.0\n",
    "Epoch 727 Reward 500.0\n",
    "Epoch 596 Reward 270.0\n",
    "Epoch 619 Reward 370.0\n",
    "Epoch 815 Reward 340.0\n",
    "Epoch 607 Reward 290.0\n",
    "Epoch 585 Reward 390.0\n",
    "Epoch 539 Reward 250.0\n",
    "Epoch 712 Reward 440.0\n",
    "Epoch 481 Reward 170.0\n",
    "Epoch 619 Reward 250.0\n",
    "Epoch 674 Reward 420.0\n",
    "Epoch 643 Reward 400.0\n",
    "Epoch 837 Reward 660.0\n",
    "Epoch 1433 Reward 1480.0\n",
    "Epoch 694 Reward 380.0\n",
    "Epoch 550 Reward 290.0\n",
    "Epoch 848 Reward 400.0\n",
    "Epoch 826 Reward 750.0\n",
    "Epoch 682 Reward 570.0\n",
    "Epoch 934 Reward 890.0\n",
    "Epoch 706 Reward 570.0\n",
    "Epoch 908 Reward 700.0\n",
    "Epoch 1283 Reward 860.0\n",
    "Epoch 886 Reward 600.0\n",
    "Epoch 832 Reward 520.0\n",
    "Epoch 484 Reward 260.0\n",
    "Epoch 1112 Reward 920.0\n",
    "Epoch 665 Reward 300.0\n",
    "Epoch 817 Reward 1190.0\n",
    "Epoch 1045 Reward 770.0\n",
    "Epoch 833 Reward 540.0\n",
    "Epoch 494 Reward 330.0\n",
    "Epoch 557 Reward 270.0\n",
    "Epoch 555 Reward 220.0\n",
    "Epoch 864 Reward 1200.0\n",
    "Epoch 923 Reward 780.0\n",
    "Epoch 542 Reward 270.0\n",
    "Epoch 501 Reward 190.0\n",
    "Epoch 1208 Reward 1610.0\n",
    "Epoch 818 Reward 690.0\n",
    "Epoch 1086 Reward 1830.0\n",
    "Epoch 720 Reward 350.0\n",
    "Epoch 604 Reward 440.0\n",
    "Epoch 497 Reward 290.0\n",
    "Epoch 752 Reward 300.0\n",
    "Epoch 687 Reward 400.0\n",
    "Epoch 917 Reward 870.0\n",
    "Epoch 1086 Reward 960.0\n",
    "Epoch 918 Reward 670.0\n",
    "Epoch 1049 Reward 740.0\n",
    "Epoch 978 Reward 690.0\n",
    "Epoch 494 Reward 260.0\n",
    "Epoch 702 Reward 630.0\n",
    "Epoch 834 Reward 530.0\n",
    "Epoch 632 Reward 360.0\n",
    "Epoch 777 Reward 690.0\n",
    "Epoch 671 Reward 570.0\n",
    "Epoch 889 Reward 690.0\n",
    "Epoch 807 Reward 590.0\n",
    "Epoch 466 Reward 260.0\n",
    "Epoch 983 Reward 540.0\n",
    "Epoch 548 Reward 250.0\n",
    "Epoch 509 Reward 200.0\n",
    "Epoch 506 Reward 290.0\n",
    "Epoch 650 Reward 400.0\n",
    "Epoch 918 Reward 700.0\n",
    "Epoch 821 Reward 460.0\n",
    "Epoch 702 Reward 350.0\n",
    "Epoch 595 Reward 430.0\n",
    "Epoch 750 Reward 530.0\n",
    "Epoch 896 Reward 340.0\n",
    "Epoch 1265 Reward 1010.0\n",
    "Epoch 760 Reward 520.0\n",
    "Epoch 570 Reward 250.0\n",
    "Epoch 555 Reward 230.0\n",
    "Epoch 685 Reward 420.0\n",
    "Epoch 581 Reward 390.0\n",
    "Epoch 780 Reward 480.0\n",
    "Epoch 836 Reward 880.0\n",
    "Epoch 503 Reward 310.0\n",
    "Epoch 720 Reward 610.0\n",
    "Epoch 483 Reward 300.0\n",
    "Epoch 732 Reward 370.0\n",
    "Epoch 972 Reward 1020.0\n",
    "Epoch 563 Reward 190.0\n",
    "Epoch 438 Reward 160.0\n",
    "Epoch 435 Reward 160.0\n",
    "Epoch 707 Reward 420.0\n",
    "Epoch 542 Reward 170.0\n",
    "Epoch 453 Reward 230.0\n",
    "Epoch 684 Reward 730.0\n",
    "Epoch 924 Reward 720.0\n",
    "Epoch 835 Reward 450.0\n",
    "Epoch 828 Reward 430.0\n",
    "Epoch 820 Reward 660.0\n",
    "Epoch 1089 Reward 1040.0\n",
    "Epoch 698 Reward 580.0\n",
    "Epoch 324 Reward 70.0\n",
    "Epoch 968 Reward 940.0\n",
    "Epoch 480 Reward 240.0\n",
    "Epoch 625 Reward 270.0\n",
    "Epoch 786 Reward 570.0\n",
    "Epoch 471 Reward 240.0\n",
    "Epoch 1117 Reward 1200.0\n",
    "Epoch 464 Reward 290.0\n",
    "Epoch 713 Reward 270.0\n",
    "Epoch 712 Reward 360.0\n",
    "Epoch 639 Reward 440.0\n",
    "Epoch 768 Reward 490.0\n",
    "Epoch 547 Reward 270.0\n",
    "Epoch 944 Reward 610.0\n",
    "Epoch 1070 Reward 1970.0\n",
    "Epoch 1116 Reward 1140.0\n",
    "Epoch 661 Reward 210.0\n",
    "Epoch 529 Reward 260.0\n",
    "Epoch 597 Reward 400.0\n",
    "Epoch 512 Reward 260.0\n",
    "Epoch 672 Reward 410.0\n",
    "Epoch 681 Reward 350.0\n",
    "Epoch 554 Reward 260.0\n",
    "Epoch 1231 Reward 890.0\n",
    "Epoch 627 Reward 310.0\n",
    "Epoch 711 Reward 450.0\n",
    "Epoch 509 Reward 330.0\n",
    "Epoch 861 Reward 360.0\n",
    "Epoch 835 Reward 590.0\n",
    "Epoch 677 Reward 1900.0\n",
    "Epoch 867 Reward 640.0\n",
    "Epoch 922 Reward 520.0\n",
    "Epoch 666 Reward 300.0\n",
    "Epoch 903 Reward 490.0\n",
    "Epoch 742 Reward 450.0\n",
    "Epoch 658 Reward 410.0\n",
    "Epoch 976 Reward 510.0\n",
    "Epoch 874 Reward 790.0\n",
    "Epoch 508 Reward 240.0\n",
    "Epoch 850 Reward 960.0\n",
    "Epoch 962 Reward 1150.0\n",
    "Epoch 752 Reward 280.0\n",
    "Epoch 1199 Reward 1120.0\n",
    "Epoch 472 Reward 270.0\n",
    "Epoch 619 Reward 400.0\n",
    "Epoch 502 Reward 350.0\n",
    "Epoch 670 Reward 380.0\n",
    "Epoch 643 Reward 510.0\n",
    "Epoch 780 Reward 420.0\n",
    "Epoch 523 Reward 160.0\n",
    "Epoch 882 Reward 540.0\n",
    "Epoch 1420 Reward 770.0\n",
    "Epoch 649 Reward 430.0\n",
    "Epoch 515 Reward 410.0\n",
    "Epoch 572 Reward 220.0\n",
    "Epoch 993 Reward 1140.0\n",
    "Epoch 649 Reward 500.0\n",
    "Epoch 777 Reward 500.0\n",
    "Epoch 882 Reward 310.0\n",
    "Epoch 482 Reward 220.0\n",
    "Epoch 469 Reward 280.0\n",
    "Epoch 798 Reward 470.0\n",
    "Epoch 780 Reward 320.0\n",
    "Epoch 613 Reward 470.0\n",
    "Epoch 447 Reward 110.0\n",
    "Epoch 797 Reward 600.0\n",
    "Epoch 590 Reward 270.0\n",
    "Epoch 1011 Reward 1380.0\n",
    "Epoch 919 Reward 700.0\n",
    "Epoch 1026 Reward 970.0\n",
    "Epoch 667 Reward 430.0\n",
    "Epoch 832 Reward 630.0\n",
    "Epoch 623 Reward 480.0\n",
    "Epoch 1073 Reward 860.0\n",
    "Epoch 521 Reward 430.0\n",
    "Epoch 1097 Reward 920.0\n",
    "Epoch 607 Reward 260.0\n",
    "Epoch 636 Reward 260.0\n",
    "Epoch 850 Reward 560.0\n",
    "Epoch 487 Reward 280.0\n",
    "Epoch 751 Reward 590.0\n",
    "Epoch 844 Reward 1180.0\n",
    "Epoch 663 Reward 440.0\n",
    "Epoch 964 Reward 780.0\n",
    "Epoch 858 Reward 750.0\n",
    "Epoch 535 Reward 330.0\n",
    "Epoch 572 Reward 300.0\n",
    "Epoch 857 Reward 1170.0\n",
    "Epoch 1240 Reward 870.0\n",
    "Epoch 777 Reward 550.0\n",
    "Epoch 906 Reward 800.0\n",
    "Epoch 1148 Reward 1470.0\n",
    "Epoch 953 Reward 500.0\n",
    "Epoch 574 Reward 360.0\n",
    "Epoch 693 Reward 350.0\n",
    "Epoch 682 Reward 520.0\n",
    "Epoch 525 Reward 320.0\n",
    "Epoch 638 Reward 530.0\n",
    "Epoch 1427 Reward 2420.0\n",
    "Epoch 909 Reward 880.0\n",
    "Epoch 1173 Reward 1300.0\n",
    "Epoch 631 Reward 510.0\n",
    "Epoch 411 Reward 250.0\n",
    "Epoch 843 Reward 410.0\n",
    "Epoch 1516 Reward 790.0\n",
    "Epoch 790 Reward 590.0\n",
    "Epoch 577 Reward 250.0\n",
    "Epoch 470 Reward 290.0\n",
    "Epoch 1014 Reward 930.0\n",
    "Epoch 554 Reward 330.0\n",
    "Epoch 916 Reward 1210.0\n",
    "Epoch 531 Reward 360.0\n",
    "Epoch 1203 Reward 770.0\n",
    "Epoch 571 Reward 250.0\n",
    "Epoch 1172 Reward 710.0\n",
    "Epoch 487 Reward 310.0\n",
    "Epoch 754 Reward 520.0\n",
    "Epoch 1178 Reward 1920.0\n",
    "Epoch 689 Reward 610.0\n",
    "Epoch 419 Reward 230.0\n",
    "Epoch 682 Reward 410.0\n",
    "Epoch 646 Reward 360.0\n",
    "Epoch 707 Reward 320.0\n",
    "Epoch 546 Reward 360.0\n",
    "Epoch 491 Reward 420.0\n",
    "Epoch 758 Reward 520.0\n",
    "Epoch 771 Reward 600.0\n",
    "Epoch 787 Reward 530.0\n",
    "Epoch 683 Reward 440.0\n",
    "Epoch 711 Reward 610.0\n",
    "Epoch 587 Reward 310.0\n",
    "Epoch 1082 Reward 660.0\n",
    "Epoch 517 Reward 290.0\n",
    "Epoch 435 Reward 210.0\n",
    "Epoch 689 Reward 570.0\n",
    "Epoch 839 Reward 440.0\n",
    "Epoch 800 Reward 960.0\n",
    "Epoch 455 Reward 190.0\n",
    "Epoch 845 Reward 510.0\n",
    "Epoch 703 Reward 960.0\n",
    "Epoch 958 Reward 1080.0\n",
    "Epoch 463 Reward 180.0\n",
    "Epoch 559 Reward 350.0\n",
    "Epoch 486 Reward 320.0\n",
    "Epoch 1103 Reward 860.0\n",
    "Epoch 718 Reward 360.0\n",
    "Epoch 1117 Reward 1170.0\n",
    "Epoch 654 Reward 330.0\n",
    "Epoch 939 Reward 490.0\n",
    "Epoch 710 Reward 240.0\n",
    "Epoch 835 Reward 890.0\n",
    "Epoch 868 Reward 690.0\n",
    "Epoch 579 Reward 220.0\n",
    "Epoch 580 Reward 330.0\n",
    "Epoch 410 Reward 70.0\n",
    "Epoch 500 Reward 270.0\n",
    "Epoch 993 Reward 590.0\n",
    "Epoch 1159 Reward 580.0\n",
    "Epoch 582 Reward 370.0\n",
    "Epoch 361 Reward 110.0\n",
    "Epoch 703 Reward 430.0\n",
    "Epoch 920 Reward 1300.0\n",
    "Epoch 867 Reward 1210.0\n",
    "Epoch 572 Reward 320.0\n",
    "Epoch 1129 Reward 1550.0\n",
    "Epoch 712 Reward 250.0\n",
    "Epoch 579 Reward 460.0\n",
    "Epoch 796 Reward 450.0\n",
    "Epoch 980 Reward 460.0\n",
    "Epoch 1047 Reward 1300.0\n",
    "Epoch 911 Reward 500.0\n",
    "Epoch 617 Reward 510.0\n",
    "Epoch 649 Reward 560.0\n",
    "Epoch 547 Reward 370.0\n",
    "Epoch 867 Reward 750.0\n",
    "Epoch 1449 Reward 1540.0\n",
    "Epoch 967 Reward 510.0\n",
    "Epoch 792 Reward 420.0\n",
    "Epoch 786 Reward 610.0\n",
    "Epoch 606 Reward 440.0\n",
    "Epoch 742 Reward 590.0\n",
    "Epoch 472 Reward 150.0\n",
    "Epoch 814 Reward 720.0\n",
    "Epoch 795 Reward 1130.0\n",
    "Epoch 1357 Reward 1520.0\n",
    "Epoch 624 Reward 370.0\n",
    "Epoch 841 Reward 1070.0\n",
    "Epoch 520 Reward 360.0\n",
    "Epoch 662 Reward 480.0\n",
    "Epoch 673 Reward 380.0\n",
    "Epoch 527 Reward 230.0\n",
    "Epoch 749 Reward 330.0\n",
    "Epoch 1337 Reward 1460.0\n",
    "Epoch 1012 Reward 680.0\n",
    "Epoch 730 Reward 440.0\n",
    "Epoch 724 Reward 530.0\n",
    "Epoch 989 Reward 2070.0\n",
    "Epoch 880 Reward 560.0\n",
    "Epoch 697 Reward 350.0\n",
    "Epoch 507 Reward 240.0\n",
    "Epoch 954 Reward 720.0\n",
    "Epoch 652 Reward 430.0\n",
    "Epoch 747 Reward 570.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
